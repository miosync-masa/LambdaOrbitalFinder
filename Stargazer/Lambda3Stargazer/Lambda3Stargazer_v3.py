"""
Pure LambdaÂ³ Framework - Topological Structure Analysis
NO TIME, NO PHYSICS, ONLY STRUCTURE!

This revolutionary approach detects hidden structures using only topological
properties of observation sequences - no physical constants required.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.optimize import curve_fit, minimize
from scipy.signal import find_peaks, correlate, hilbert, savgol_filter
from scipy.interpolate import interp1d
from scipy.fft import fft, fftfreq
from scipy.ndimage import gaussian_filter1d, median_filter
import argparse
from typing import Tuple, Dict, List, Optional
import warnings
warnings.filterwarnings('ignore')


class PureLambda3Analyzer:
    """
    Pure LambdaÂ³ Framework - Topological Structure Analysis

    COMPLETELY PHYSICS-FREE VERSION!
    No G, no masses, no Kepler's laws - just pure structure!
    """

    # Structural constants (no physical meaning)
    STRUCTURAL_RECURRENCE_FACTORS = [0.5, 0.67, 0.75, 1.0, 1.33, 1.5, 2.0]
    TOPOLOGICAL_COHERENCE_THRESHOLD = 0.15

    def __init__(self, verbose: bool = True):
        """Initialize the Pure LambdaÂ³ Analyzer."""
        self.verbose = verbose
        self.results = {}
        self.adaptive_params = None

    def load_and_clean_data(self, filename: str) -> Tuple[pd.DataFrame, np.ndarray]:
        """Load and clean observational data."""
        if self.verbose:
            print(f"ğŸ“Š Loading observational data from {filename}...")

        df = pd.read_csv(filename, index_col='step')

        # Count missing values
        missing_x = df['x_noisy'].isna().sum()
        missing_y = df['y_noisy'].isna().sum()
        total_points = len(df)

        if self.verbose:
            print(f"   Data points: {total_points}")
            print(f"   Missing: X={missing_x} ({missing_x/total_points*100:.1f}%), " +
                  f"Y={missing_y} ({missing_y/total_points*100:.1f}%)")

        # Interpolate missing values with cubic splines
        df['x_clean'] = df['x_noisy'].interpolate(method='cubic', limit_direction='both')
        df['y_clean'] = df['y_noisy'].interpolate(method='cubic', limit_direction='both')
        df['z_clean'] = df['z'].fillna(0)

        # Extract clean positions
        positions = df[['x_clean', 'y_clean', 'z_clean']].values

        return df, positions

    def compute_lambda_structures(self, positions: np.ndarray) -> Dict[str, np.ndarray]:
        """
        Compute fundamental LambdaÂ³ structural quantities from observation sequence.
        """
        if self.verbose:
            print("\nğŸŒŒ Computing LambdaÂ³ structural tensors from observation steps...")

        n_steps = len(positions)

        # 1. Î›F - Structural flow field (è¦³æ¸¬ã‚¹ãƒ†ãƒƒãƒ—é–“ã®æ§‹é€ å¤‰åŒ–)
        lambda_F = np.zeros((n_steps-1, 3))
        lambda_F_mag = np.zeros(n_steps-1)

        for step in range(n_steps-1):
            lambda_F[step] = positions[step+1] - positions[step]
            lambda_F_mag[step] = np.linalg.norm(lambda_F[step])

        # 2. Î›FF - Second-order structure (æ§‹é€ å¤‰åŒ–ã®å¤‰åŒ–)
        lambda_FF = np.zeros((n_steps-2, 3))
        lambda_FF_mag = np.zeros(n_steps-2)

        for step in range(n_steps-2):
            lambda_FF[step] = lambda_F[step+1] - lambda_F[step]
            lambda_FF_mag[step] = np.linalg.norm(lambda_FF[step])

        # 3. ÏT - Tension field (å±€æ‰€çš„ãªæ§‹é€ ã®å¼µåŠ›)
        window_steps = max(3, n_steps // 200)  # è¦³æ¸¬ã‚¹ãƒ†ãƒƒãƒ—ã®0.5%
        rho_T = np.zeros(n_steps)

        for step in range(n_steps):
            start_step = max(0, step - window_steps)
            end_step = min(n_steps, step + window_steps + 1)
            local_positions = positions[start_step:end_step]

            if len(local_positions) > 1:
                centered = local_positions - np.mean(local_positions, axis=0)
                cov = np.cov(centered.T)
                rho_T[step] = np.trace(cov)

        # 4. Q_Î› - Topological charge (ä½ç›¸çš„å·»ãæ•°ã®å¤‰åŒ–)
        Q_lambda = np.zeros(n_steps-1)

        for step in range(1, n_steps-1):
            if lambda_F_mag[step] > 1e-10 and lambda_F_mag[step-1] > 1e-10:
                v1 = lambda_F[step-1] / lambda_F_mag[step-1]
                v2 = lambda_F[step] / lambda_F_mag[step]

                cos_angle = np.clip(np.dot(v1, v2), -1, 1)
                angle = np.arccos(cos_angle)

                # 2Då¹³é¢ã§ã®å›è»¢æ–¹å‘
                cross_z = v1[0]*v2[1] - v1[1]*v2[0]
                signed_angle = angle if cross_z >= 0 else -angle

                Q_lambda[step] = signed_angle / (2 * np.pi)

        # 5. Helicity (æ§‹é€ ã®ã­ã˜ã‚Œ)
        helicity = np.zeros(n_steps-1)
        for step in range(n_steps-1):
            if step > 0:
                r = positions[step]
                v = lambda_F[step-1]
                if np.linalg.norm(r) > 0 and np.linalg.norm(v) > 0:
                    helicity[step] = np.dot(r, v) / (np.linalg.norm(r) * np.linalg.norm(v))

        if self.verbose:
            print(f"   Computed tensors for {n_steps} observation steps")
            print(f"   Î›F dimension: {lambda_F.shape}")
            print(f"   Q_Î› total winding: {np.sum(Q_lambda):.3f}")
            print(f"   Mean tension ÏT: {np.mean(rho_T):.3f}")

        return {
            'lambda_F': lambda_F,
            'lambda_F_mag': lambda_F_mag,
            'lambda_FF': lambda_FF,
            'lambda_FF_mag': lambda_FF_mag,
            'rho_T': rho_T,
            'Q_lambda': Q_lambda,
            'Q_cumulative': np.cumsum(Q_lambda),
            'helicity': helicity,
            'positions': positions,
            'n_observation_steps': n_steps
        }

    def detect_topological_breaks(self, structures: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:
        """
        Detect topological breaks and anomalies in observation sequence
        """
        Q_cumulative = structures['Q_cumulative']
        lambda_F_mag = structures['lambda_F_mag']
        lambda_FF_mag = structures['lambda_FF_mag']
        rho_T = structures['rho_T']
        n_steps = structures['n_observation_steps']

        # è¦³æ¸¬çª“ã®ã‚µã‚¤ã‚ºï¼ˆã‚¹ãƒ†ãƒƒãƒ—æ•°ã®1%ï¼‰
        window_steps = max(5, n_steps // 100)

        if self.verbose:
            print("\nğŸ” Detecting topological breaks in observation sequence...")
            print(f"   Total observation steps: {n_steps}")
            print(f"   Analysis window: {window_steps} steps")

        # 1. ãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«ãƒãƒ£ãƒ¼ã‚¸ã®ç ´ã‚Œ
        if len(Q_cumulative) > 20:
            # Savitzky-Golayãƒ•ã‚£ãƒ«ã‚¿ã§æ»‘ã‚‰ã‹ãªæˆåˆ†ã‚’æŠ½å‡º
            window_length = min(15, len(Q_cumulative)//15*2+1)
            if window_length % 2 == 0:
                window_length += 1

            from scipy.signal import savgol_filter
            Q_smooth = savgol_filter(Q_cumulative,
                                  window_length=window_length,
                                  polyorder=3)
            Q_residual = Q_cumulative - Q_smooth
        else:
            Q_residual = Q_cumulative - np.mean(Q_cumulative)

        # 2. æ§‹é€ ãƒ•ãƒ­ãƒ¼ï¼ˆÎ›Fï¼‰ã®ç•°å¸¸æ¤œå‡º
        lambda_F_anomaly = np.zeros_like(lambda_F_mag)
        for step in range(len(lambda_F_mag)):
            start = max(0, step - window_steps)
            end = min(len(lambda_F_mag), step + window_steps + 1)

            local_mean = np.mean(lambda_F_mag[start:end])
            local_std = np.std(lambda_F_mag[start:end])

            if local_std > 0:
                # å±€æ‰€çš„ãªæ¨™æº–åŒ–
                lambda_F_anomaly[step] = (lambda_F_mag[step] - local_mean) / local_std

        # 3. æ§‹é€ åŠ é€Ÿåº¦ï¼ˆÎ›FFï¼‰ã®ç•°å¸¸
        accel_window = max(3, window_steps // 2)
        lambda_FF_anomaly = np.zeros_like(lambda_FF_mag)

        for step in range(len(lambda_FF_mag)):
            start = max(0, step - accel_window)
            end = min(len(lambda_FF_mag), step + accel_window + 1)

            local_mean = np.mean(lambda_FF_mag[start:end])
            local_std = np.std(lambda_FF_mag[start:end])

            if local_std > 0:
                lambda_FF_anomaly[step] = (lambda_FF_mag[step] - local_mean) / local_std

        # 4. ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å ´ã®è·³èº
        from scipy.ndimage import gaussian_filter1d
        rho_T_smooth = gaussian_filter1d(rho_T, sigma=window_steps/3)
        rho_T_breaks = np.abs(rho_T - rho_T_smooth)

        # 5. è¤‡åˆçš„ãªç•°å¸¸ã‚¹ã‚³ã‚¢
        # å„æˆåˆ†ã®é•·ã•ã‚’åˆã‚ã›ã‚‹
        min_len = min(len(Q_residual), len(lambda_F_anomaly),
                    len(lambda_FF_anomaly), len(rho_T_breaks)-1)

        # é‡ã¿ä»˜ãåˆæˆï¼ˆãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«ãªé‡è¦åº¦ã«åŸºã¥ãï¼‰
        combined_anomaly = (
            np.abs(Q_residual[:min_len]) * 3.0 +        # Q_Î›ã®ç ´ã‚ŒãŒæœ€é‡è¦
            np.abs(lambda_F_anomaly[:min_len]) * 1.5 +  # ãƒ•ãƒ­ãƒ¼ã®ç•°å¸¸
            np.abs(lambda_FF_anomaly[:min_len]) * 2.0 + # åŠ é€Ÿåº¦ã®ç•°å¸¸
            rho_T_breaks[:min_len] * 1.5                # ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®è·³èº
        ) / 8.0

        # ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã®çµ±è¨ˆ
        if self.verbose:
            n_high_anomaly = np.sum(combined_anomaly > np.mean(combined_anomaly) + 2*np.std(combined_anomaly))
            print(f"   High anomaly steps: {n_high_anomaly} ({n_high_anomaly/len(combined_anomaly)*100:.1f}%)")

            # å„æˆåˆ†ã®å¯„ä¸
            contributions = {
                'Q_residual': np.mean(np.abs(Q_residual[:min_len])),
                'lambda_F': np.mean(np.abs(lambda_F_anomaly[:min_len])),
                'lambda_FF': np.mean(np.abs(lambda_FF_anomaly[:min_len])),
                'rho_T': np.mean(rho_T_breaks[:min_len])
            }

            max_contrib = max(contributions.values())
            print("   Anomaly contributions:")
            for name, value in contributions.items():
                print(f"     {name}: {value/max_contrib*100:.0f}%")

        return {
            'Q_residual': Q_residual,
            'lambda_F_anomaly': lambda_F_anomaly,
            'lambda_FF_anomaly': lambda_FF_anomaly,
            'rho_T_breaks': rho_T_breaks,
            'combined_anomaly': combined_anomaly,
            'window_steps': window_steps,
            'n_observation_steps': n_steps
        }

    def detect_structural_boundaries(self, structures: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:
        """
        Detect pure structural boundaries in observation sequence
        """
        if self.verbose:
            print("\nğŸŒŸ Detecting structural boundaries in observation sequence...")

        Q_cumulative = structures['Q_cumulative']
        lambda_F = structures['lambda_F']
        rho_T = structures['rho_T']
        n_steps = len(structures['positions'])

        # 1. Q_Î›ã®ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒè§£æ
        def compute_local_fractal_dimension(series, window_steps=30):
            """è¦³æ¸¬çª“å†…ã§ã®ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒã‚’è¨ˆç®—"""
            dims = []
            for step in range(window_steps, len(series) - window_steps):
                local = series[step-window_steps:step+window_steps]

                # Box-counting for 1D series
                scales = [2, 4, 8, 16]
                counts = []
                for scale in scales:
                    boxes = 0
                    for j in range(0, len(local)-scale, scale):
                        segment = local[j:j+scale]
                        if np.ptp(segment) > 0:
                            boxes += 1
                    counts.append(boxes if boxes > 0 else 1)

                if len(counts) > 1 and max(counts) > min(counts):
                    log_scales = np.log(scales[:len(counts)])
                    log_counts = np.log(counts)
                    slope = np.polyfit(log_scales, log_counts, 1)[0]
                    dims.append(-slope)
                else:
                    dims.append(1.0)

            return np.array(dims)

        # 2. Î›Fã®å¤šã‚¹ã‚±ãƒ¼ãƒ«æ§‹é€ çš„ä¸€è²«æ€§
        def compute_structural_coherence(lambda_F, scale_steps=[5, 10, 20, 40]):
            """ç•°ãªã‚‹è¦³æ¸¬ã‚¹ã‚±ãƒ¼ãƒ«ã§ã®æ§‹é€ ã®ä¸€è²«æ€§"""
            coherences = []

            for scale in scale_steps:
                if scale >= len(lambda_F):
                    continue

                coherence_values = []
                for step in range(scale, len(lambda_F) - scale):
                    # éå»ã¨æœªæ¥ã®å±€æ‰€ãƒ™ã‚¯ãƒˆãƒ«
                    v_past = lambda_F[step-scale:step]
                    v_future = lambda_F[step:step+scale]

                    past_mean = np.mean(v_past, axis=0)
                    future_mean = np.mean(v_future, axis=0)

                    if np.linalg.norm(past_mean) > 0 and np.linalg.norm(future_mean) > 0:
                        coherence = np.dot(past_mean, future_mean) / (
                            np.linalg.norm(past_mean) * np.linalg.norm(future_mean)
                        )
                        coherence_values.append(coherence)

                if coherence_values:
                    coherences.append(np.array(coherence_values))

            return coherences

        # 3. ãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«çµåˆå¼·åº¦
        def compute_coupling_strength(Q_series, window_steps=50):
            """è¦³æ¸¬çª“å†…ã§ã®æ§‹é€ çš„çµåˆã®å¼·ã•"""
            n = len(Q_series)
            coupling = np.zeros(n)

            for step in range(window_steps, n - window_steps):
                local_Q = Q_series[step-window_steps:step+window_steps]

                local_var = np.var(np.diff(local_Q))
                global_var = np.var(np.diff(Q_series))

                if global_var > 0:
                    coupling[step] = 1 - np.abs(local_var - global_var) / global_var
                else:
                    coupling[step] = 1.0

            return np.clip(coupling, 0, 1)

        # 4. æ§‹é€ çš„ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼å‹¾é…
        def compute_structural_entropy(rho_T, window_steps=30):
            """ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å ´ã®æƒ…å ±ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼"""
            entropy = np.zeros(len(rho_T))

            for step in range(window_steps, len(rho_T) - window_steps):
                local_rho = rho_T[step-window_steps:step+window_steps]

                if np.sum(local_rho) > 0:
                    p = local_rho / np.sum(local_rho)
                    entropy[step] = -np.sum(p * np.log(p + 1e-10))

            return entropy

        # å…¨ã¦ã®æ§‹é€ çš„æŒ‡æ¨™ã‚’è¨ˆç®—
        window_steps = max(30, n_steps // 50)  # è¦³æ¸¬ã‚¹ãƒ†ãƒƒãƒ—ã®2%
        fractal_dims = compute_local_fractal_dimension(Q_cumulative, window_steps)
        coherences = compute_structural_coherence(lambda_F)
        coupling = compute_coupling_strength(Q_cumulative, window_steps)
        entropy = compute_structural_entropy(rho_T, window_steps)

        # æœ€å°é•·ã«æ­£è¦åŒ–
        min_len = min(len(fractal_dims), len(coupling), len(entropy))
        if coherences and len(coherences[0]) > 0:
            min_len = min(min_len, len(coherences[0]))

        # å„æŒ‡æ¨™ã®å‹¾é…ã‚’è¨ˆç®—
        if len(fractal_dims) > 1:
            fractal_gradient = np.abs(np.gradient(fractal_dims[:min_len]))
        else:
            fractal_gradient = np.zeros(min_len)

        if coherences and len(coherences[0]) > 0:
            coherence_signal = coherences[0]
            coherence_drop = 1 - coherence_signal[:min_len]
        else:
            coherence_drop = np.zeros(min_len)

        coupling_weakness = 1 - coupling[:min_len]

        if len(entropy) > 1:
            entropy_gradient = np.abs(np.gradient(entropy[:min_len]))
        else:
            entropy_gradient = np.zeros(min_len)

        # å¢ƒç•Œã‚¹ã‚³ã‚¢ã®åˆæˆ
        boundary_score = (
            2.0 * fractal_gradient +      # ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒã®å¤‰åŒ–
            1.5 * coherence_drop +        # æ§‹é€ çš„ä¸€è²«æ€§ã®ä½ä¸‹
            1.0 * coupling_weakness +     # çµåˆã®å¼±ã¾ã‚Š
            1.0 * entropy_gradient        # æƒ…å ±éšœå£
        ) / 5.5

        # å¢ƒç•Œä½ç½®ã®æ¤œå‡ºï¼ˆè¦³æ¸¬ã‚¹ãƒ†ãƒƒãƒ—å˜ä½ï¼‰
        if len(boundary_score) > 10:
            min_distance_steps = max(50, n_steps // 30)  # è¦³æ¸¬ã‚¹ãƒ†ãƒƒãƒ—ã®3.3%
            peaks, properties = find_peaks(
                boundary_score,
                height=np.mean(boundary_score) + np.std(boundary_score),
                distance=min_distance_steps
            )
        else:
            peaks = np.array([])

        if self.verbose:
            print(f"   Found {len(peaks)} structural boundaries")
            if len(peaks) > 0:
                print(f"   Boundary locations (steps): {peaks[:5].tolist()}...")
                strengths = boundary_score[peaks[:5]]
                strength_str = ", ".join([f"{s:.3f}" for s in strengths])
                print(f"   Boundary strengths: [{strength_str}]...")

        return {
            'boundary_score': boundary_score,
            'boundary_locations': peaks,
            'fractal_dimension': fractal_dims,
            'structural_coherence': coherences[0] if coherences else np.array([]),
            'coupling_strength': coupling,
            'structural_entropy': entropy,
            'n_observation_steps': n_steps
        }

    def filter_harmonics_in_recurrence(self, recurrence_patterns: List[Dict]) -> List[Dict]:
        """
        é«˜èª¿æ³¢ã‚’é™¤å¤–ã—ã¦åŸºæœ¬çš„ãªå†å¸°ãƒ‘ã‚¿ãƒ¼ãƒ³ã ã‘ã‚’æŠ½å‡º
        """
        if not recurrence_patterns:
            return recurrence_patterns

        filtered = []
        used = set()

        # è¦³æ¸¬é–“éš”ã§ã‚½ãƒ¼ãƒˆï¼ˆé•·ã„é †ï¼‰
        sorted_patterns = sorted(recurrence_patterns,
                              key=lambda x: x['observation_interval'],
                              reverse=True)

        for i, pattern in enumerate(sorted_patterns):
            if i in used:
                continue

            # ã“ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åŸºæœ¬æ³¢ã¨ã—ã¦æ¡ç”¨
            filtered.append(pattern)
            used.add(i)

            # ã“ã®åŸºæœ¬æ³¢ã®é«˜èª¿æ³¢ã‚’é™¤å¤–
            base_interval = pattern['observation_interval']

            for j, other in enumerate(sorted_patterns):
                if j in used or j == i:
                    continue

                # æ•´æ•°æ¯”ã‚’ãƒã‚§ãƒƒã‚¯
                ratio = base_interval / other['observation_interval']

                # 2, 3, 4, 5å€ã®é«˜èª¿æ³¢ãªã‚‰é™¤å¤–
                for n in [2, 3, 4, 5]:
                    if abs(ratio - n) < 0.1:
                        used.add(j)
                        if self.verbose:
                            print(f"   Filtered harmonic: {other['observation_interval']:.0f} steps "
                                  f"(n={n} of {base_interval:.0f})")
                        break

        return filtered

    def extract_topological_recurrence(self, structures: Dict[str, np.ndarray]) -> List[Dict]:
        """
        Extract STRUCTURAL recurrence patterns from observation sequence
        """
        if self.verbose:
            print("\nğŸŒŒ Extracting topological recurrence patterns from observation steps...")

        Q_cumulative = structures['Q_cumulative']
        lambda_F = structures['lambda_F']
        rho_T = structures['rho_T']
        n_steps = len(Q_cumulative)  # è¦³æ¸¬ã‚¹ãƒ†ãƒƒãƒ—æ•°

        recurrence_patterns = []

        # 1. Q_Î›ã®å·»ãæ•°ã«ã‚ˆã‚‹æ§‹é€ çš„å›å¸°
        Q_initial = Q_cumulative[0] if n_steps > 0 else 0
        for step_i in range(n_steps//10, n_steps):
            Q_diff = abs(Q_cumulative[step_i] - Q_initial - round(Q_cumulative[step_i] - Q_initial))
            if Q_diff < 0.1:  # ã»ã¼æ•´æ•°å·»ã
                recurrence_patterns.append({
                    'observation_interval': step_i,  # åˆæœŸã‹ã‚‰ã®è¦³æ¸¬ã‚¹ãƒ†ãƒƒãƒ—æ•°
                    'winding_number': round(Q_cumulative[step_i] - Q_initial),
                    'structural_coherence': 1.0 - Q_diff,
                    'pattern_type': 'Q_winding'
                })

        # 2. Î›Fãƒ™ã‚¯ãƒˆãƒ«å ´ã®ä½ç›¸ç©ºé–“å›å¸°
        for current_step in range(n_steps//4, len(lambda_F)):
            for past_step in range(min(current_step//2, len(lambda_F))):
                if current_step < len(lambda_F) and past_step < len(lambda_F):
                    # ä½ç›¸ç©ºé–“ã§ã®æ§‹é€ çš„è·é›¢
                    phase_distance = np.linalg.norm(lambda_F[current_step] - lambda_F[past_step])

                    if phase_distance < np.std(structures['lambda_F_mag']) * 0.2:
                        recurrence_patterns.append({
                            'observation_interval': current_step - past_step,  # ã‚¹ãƒ†ãƒƒãƒ—é–“éš”
                            'phase_similarity': 1.0 / (1.0 + phase_distance),
                            'structural_coherence': np.exp(-phase_distance),
                            'pattern_type': 'phase_return'
                        })

        # 3. ÏTãƒ†ãƒ³ã‚·ãƒ§ãƒ³å ´ã®æ§‹é€ çš„æŒ¯å‹•
        rho_peaks, _ = find_peaks(rho_T, prominence=np.std(rho_T)*0.5)
        if len(rho_peaks) > 2:
            # ãƒ”ãƒ¼ã‚¯é–“ã®è¦³æ¸¬ã‚¹ãƒ†ãƒƒãƒ—æ•°
            step_intervals = np.diff(rho_peaks)
            unique_intervals, counts = np.unique(step_intervals, return_counts=True)

            for interval_steps, count in zip(unique_intervals, counts):
                if count > 2:  # 3å›ä»¥ä¸Šç¹°ã‚Šè¿”ã™ãƒ‘ã‚¿ãƒ¼ãƒ³
                    recurrence_patterns.append({
                        'observation_interval': float(interval_steps),
                        'repetitions': int(count),
                        'structural_coherence': count / len(rho_peaks),
                        'pattern_type': 'tension_oscillation'
                    })

        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã®çµ±åˆ
        consolidated = []
        tolerance = 0.15

        for pattern in recurrence_patterns:
            found = False
            for c in consolidated:
                # è¦³æ¸¬ã‚¹ãƒ†ãƒƒãƒ—é–“éš”ãŒè¿‘ã„ã‚‚ã®ã‚’çµ±åˆ
                if abs(pattern['observation_interval'] - c['observation_interval']) / c['observation_interval'] < tolerance:
                    c['structural_coherence'] = max(c['structural_coherence'], pattern['structural_coherence'])
                    c['detection_count'] = c.get('detection_count', 1) + 1
                    found = True
                    break

            if not found:
                pattern['detection_count'] = 1
                consolidated.append(pattern)

        # æ§‹é€ çš„ç¢ºä¿¡åº¦ã®è¨ˆç®—
        for p in consolidated:
            p['topological_confidence'] = p['structural_coherence'] * np.sqrt(p['detection_count'])

        consolidated.sort(key=lambda x: x['topological_confidence'], reverse=True)

        if self.verbose:
            print(f"   Found {len(consolidated)} recurrence patterns in observation sequence")
            for i, p in enumerate(consolidated[:5]):
                print(f"   {i+1}. Interval: {p['observation_interval']:.0f} steps, "
                      f"Confidence: {p['topological_confidence']:.3f}, "
                      f"Type: {p['pattern_type']}")

        return consolidated


    def decompose_structural_signatures(self, structures: Dict[str, np.ndarray],
                                      recurrence_patterns: List[Dict]) -> Dict[str, Dict]:
        """
        è¦³æ¸¬ã‚¹ãƒ†ãƒƒãƒ—åˆ—ã‹ã‚‰æ§‹é€ çš„ã‚·ã‚°ãƒãƒãƒ£ã‚’åˆ†è§£
        """
        if self.verbose:
            print("\nğŸŒ€ Decomposing structural signatures from observation sequence...")

        # è¦³æ¸¬ã‚¹ãƒ†ãƒƒãƒ—é…åˆ—
        observation_steps = np.arange(len(structures['positions']))

        structural_signatures = {}

        for i, pattern in enumerate(recurrence_patterns[:5]):
            if pattern['topological_confidence'] < 0.1:
                break

            step_interval = pattern['observation_interval']

            # æ§‹é€ çš„ãƒ¢ãƒ‡ãƒ«ï¼ˆè¦³æ¸¬ã‚¹ãƒ†ãƒƒãƒ—ã®é–¢æ•°ã¨ã—ã¦ï¼‰
            def structural_pattern(steps, amplitude, phase_offset):
                # ã‚¹ãƒ†ãƒƒãƒ—é€²è¡Œã«å¯¾ã™ã‚‹æ§‹é€ çš„å¿œç­”
                phase = 2 * np.pi * steps / step_interval + phase_offset
                return amplitude * (np.sin(phase) + 0.3 * np.sin(2*phase))

            # æ§‹é€ åï¼ˆè¦³æ¸¬é †åºãƒ™ãƒ¼ã‚¹ï¼‰
            structure_names = ['Primary_Structure', 'Secondary_Structure',
                            'Tertiary_Structure', 'Quaternary_Structure',
                            'Quinary_Structure']

            structural_signatures[structure_names[i]] = {
                'observation_interval': step_interval,  # è¦³æ¸¬ã‚¹ãƒ†ãƒƒãƒ—é–“éš”
                'topological_amplitude': pattern.get('structural_coherence', 1.0),
                'pattern_type': pattern['pattern_type'],
                'topological_confidence': pattern['topological_confidence'],
                'detection_count': pattern['detection_count']
            }

            if self.verbose:
                print(f"   {structure_names[i]}: Every {step_interval:.0f} observation steps")

        return structural_signatures

    def identify_structural_families(self, patterns: List[Dict]) -> Dict[str, List[Dict]]:
        """
        æ§‹é€ çš„ã«é–¢é€£ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        ï¼ˆè¦³æ¸¬ã‚¹ãƒ†ãƒƒãƒ—é–“éš”ã®æ•´æ•°æ¯”ã§åˆ¤å®šï¼‰
        """
        if not patterns:
            return {}

        families = {}
        used = set()

        sorted_patterns = sorted(patterns,
                              key=lambda x: x['topological_confidence'],
                              reverse=True)

        for i, pattern in enumerate(sorted_patterns):
            if i in used:
                continue

            family_key = f'structural_family_{i}'
            families[family_key] = [pattern]

            # ä»–ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ã®æ•´æ•°æ¯”ã‚’ãƒã‚§ãƒƒã‚¯
            for j, other in enumerate(sorted_patterns[i+1:], i+1):
                if j in used:
                    continue

                ratio = pattern['observation_interval'] / other['observation_interval']

                # æ•´æ•°æ¯”ãƒã‚§ãƒƒã‚¯ï¼ˆæ§‹é€ çš„é«˜èª¿æ³¢ï¼‰
                for n in [2, 3, 4, 5]:
                    if abs(ratio - n) < 0.05 or abs(ratio - 1/n) < 0.05:
                        families[family_key].append(other)
                        used.add(j)
                        break

        return families

    def merge_related_structures(self, structures_list: List[Dict]) -> List[Dict]:
        """
        é–¢é€£ã™ã‚‹æ§‹é€ ã‚’ãƒãƒ¼ã‚¸ï¼ˆé«˜èª¿æ³¢ORè¿‘æ¥å€¤ï¼‰
        """
        if len(structures_list) <= 3:
            return structures_list

        merged = []
        used = set()

        for i, s1 in enumerate(structures_list):
            if i in used:
                continue

            group = [s1]

            for j, s2 in enumerate(structures_list[i+1:], i+1):
                if j in used:
                    continue

                interval_ratio = s2['observation_interval'] / s1['observation_interval']

                # æ¡ä»¶1: é«˜èª¿æ³¢é–¢ä¿‚ï¼ˆæ•´æ•°æ¯”ï¼‰
                is_harmonic = any(
                    abs(interval_ratio - n) < 0.1 or abs(interval_ratio - 1/n) < 0.1
                    for n in [2, 3, 4, 5]
                )

                # æ¡ä»¶2: å˜ç´”ã«è¿‘ã„ï¼ˆ20%ä»¥å†…ï¼‰
                is_close = abs(interval_ratio - 1.0) < 0.2

                if is_harmonic or is_close:
                    group.append(s2)
                    used.add(j)

                    if self.verbose:
                        if is_harmonic:
                            print(f"   Harmonic relation: {s1['observation_interval']:.0f} & {s2['observation_interval']:.0f}")
                        else:
                            print(f"   Close values: {s1['observation_interval']:.0f} & {s2['observation_interval']:.0f}")

            # é«˜èª¿æ³¢ã®å ´åˆã¯åŸºæœ¬æ³¢ã‚’é¸ã¶ã€è¿‘æ¥å€¤ã®å ´åˆã¯åŠ é‡å¹³å‡
            if any(abs(s['observation_interval'] / group[0]['observation_interval'] - n) < 0.1
                  for s in group[1:] for n in [2, 3, 4, 5]):
                # é«˜èª¿æ³¢ã‚°ãƒ«ãƒ¼ãƒ—ï¼šæœ€ã‚‚é•·ã„é–“éš”ã‚’åŸºæœ¬æ³¢ã¨ã™ã‚‹
                representative = max(group, key=lambda x: x['observation_interval'])
            else:
                # è¿‘æ¥å€¤ã‚°ãƒ«ãƒ¼ãƒ—ï¼šæœ€ã‚‚ç¢ºä¿¡åº¦ã®é«˜ã„ã‚‚ã®ã‚’ä»£è¡¨ã«
                representative = max(group, key=lambda x: x['topological_confidence'])

                # åŠ é‡å¹³å‡ã§é–“éš”ã‚’æ›´æ–°
                if len(group) > 1:
                    total_conf = sum(s['topological_confidence'] for s in group)
                    representative['observation_interval'] = sum(
                        s['observation_interval'] * s['topological_confidence'] / total_conf
                        for s in group
                    )

            merged.append(representative)

        return merged

    def estimate_topological_parameters(self,
                                      structural_signatures: Dict[str, Dict],
                                      structures: Dict[str, np.ndarray]) -> List[Dict]:
        if self.verbose:
            print("\nğŸŒŒ Estimating topological parameters from observation sequence...")

        # ä¸»æ§‹é€ ã®ç‰¹æ€§ï¼ˆè¦³æ¸¬ã‚¹ãƒ†ãƒƒãƒ—ãƒ™ãƒ¼ã‚¹ï¼‰
        positions = structures['positions']
        n_observations = len(positions)
        structural_scale = np.mean(np.linalg.norm(positions, axis=1))

        # ä¸»æ§‹é€ ã®å†å¸°é–“éš”ã‚’æ¤œå‡º
        primary_interval = self.detect_primary_recurrence(structures)

        if self.verbose:
            print(f"   Observation steps: {n_observations}")
            print(f"   Primary structure: scale={structural_scale:.2f}, recurrence={primary_interval:.0f} steps")

        structures_list = []

        for name, signature in structural_signatures.items():
            # è¦³æ¸¬ã‚¹ãƒ†ãƒƒãƒ—é–“éš”
            observation_interval = signature['observation_interval']

            # æ§‹é€ çš„éšå±¤ï¼ˆä¸»æ§‹é€ ã¨ã®é–¢ä¿‚ï¼‰
            if observation_interval > primary_interval:
                hierarchy_factor = observation_interval / primary_interval
            else:
                hierarchy_factor = primary_interval / observation_interval

            # ãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«åŠå¾„ï¼ˆæ§‹é€ çš„ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
            relative_scale = (observation_interval / primary_interval) ** (2/3)
            topological_radius = structural_scale * relative_scale

            # æ§‹é€ çš„å½±éŸ¿åŠ›
            structural_influence = signature.get('topological_amplitude', 1.0) * structural_scale**2

            structure_params = {
                'name': name,
                'observation_interval': observation_interval,
                'hierarchy_factor': hierarchy_factor,
                'topological_radius': topological_radius,
                'structural_influence': structural_influence,
                'topological_impact': signature.get('topological_amplitude', 1.0),
                'topological_confidence': signature['topological_confidence'],
                'pattern_type': signature['pattern_type'],
                'detection_count': signature.get('detection_count', 1)
            }

            structures_list.append(structure_params)

        # ãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«åŠå¾„ã§ã‚½ãƒ¼ãƒˆ
        structures_list.sort(key=lambda x: x['topological_radius'])

        # ä½ç¢ºä¿¡åº¦ã‚’é™¤å¤–
        structures_list = [
            s for s in structures_list
            if s['topological_confidence'] > 0.19
        ]

        return structures_list

    def detect_primary_recurrence(self, structures: Dict[str, np.ndarray]) -> float:
        """
        ä¸»æ§‹é€ ã®å†å¸°é–“éš”ã‚’æ¤œå‡ºï¼ˆè¦³æ¸¬ã‚¹ãƒ†ãƒƒãƒ—å˜ä½ï¼‰
        """
        positions = structures['positions']
        n_steps = len(positions)

        # Method 1: Q_Î›ã®å·»ãæ•°ã‹ã‚‰
        if 'Q_cumulative' in structures:
            Q_final = structures['Q_cumulative'][-1]
            topological_winding = abs(Q_final)

            if topological_winding > 0.5:
                recurrence_interval = n_steps / topological_winding
                if n_steps * 0.3 < recurrence_interval < n_steps * 0.7:
                    return recurrence_interval

        # Method 2: æ§‹é€ çš„è‡ªå·±ç›¸ä¼¼æ€§
        structural_distances = np.linalg.norm(positions, axis=1)
        pattern = structural_distances - np.mean(structural_distances)

        # è‡ªå·±ç›¸é–¢ã§å†å¸°ã‚’æ¤œå‡º
        min_lag = int(n_steps * 0.05)  # 5%ä»¥ä¸Š
        max_lag = int(n_steps * 0.7)   # 70%ä»¥ä¸‹

        autocorr = correlate(pattern, pattern, mode='full')
        autocorr = autocorr[len(autocorr)//2:]
        autocorr = autocorr / autocorr[0]

        if max_lag > min_lag:
            peaks, _ = find_peaks(autocorr[min_lag:max_lag], height=0.3)
            if len(peaks) > 0:
                return float(peaks[0] + min_lag)

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        return float(n_steps // 3)

    def estimate_physical_parameters(self, structures: Dict[str, np.ndarray], 
                                    structural_signatures: Dict[str, Dict]) -> Dict[str, Dict]:
        """
        æ§‹é€ çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰ç‰©ç†çš„ãªå¤©ä½“ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¨å®š
        å®Œå…¨ã«ç‰©ç†å®šæ•°ãªã—ï¼ç´”ç²‹ãªãƒˆãƒãƒ­ã‚¸ãƒ¼ã‹ã‚‰å°å‡ºï¼
        """
        if self.verbose:
            print("\nğŸŒŸ Estimating physical parameters from pure topology...")
        
        physical_params = {}
        
        # ä¸»æ§‹é€ ï¼ˆä¸­å¿ƒå¤©ä½“ï¼‰ã®ç‰¹æ€§ã‚’æ¨å®š
        positions = structures['positions']
        central_scale = np.mean(np.linalg.norm(positions, axis=1))
        
        # æœ€å¤§æ§‹é€ åå·®ï¼ˆæ‘‚å‹•ã®å¼·ã•ï¼‰
        lambda_F_mag = structures['lambda_F_mag']
        baseline_flow = np.median(lambda_F_mag)
        max_deviation = np.max(lambda_F_mag) - baseline_flow
        
        if self.verbose:
            print(f"   Central structure scale: {central_scale:.3f}")
            print(f"   Maximum flow deviation: {max_deviation:.6f}")
        
        for name, signature in structural_signatures.items():
            if self.verbose:
                print(f"\n   Analyzing {name}...")
            
            # 1. è»Œé“é•·åŠå¾„ã®æ¨å®š
            # ãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«åŠå¾„ = æ§‹é€ çš„å½±éŸ¿ç¯„å›²
            a_estimated = signature.get('topological_radius', central_scale)
            
            # 2. å…¬è»¢å‘¨æœŸï¼ˆè¦³æ¸¬ã‚¹ãƒ†ãƒƒãƒ—ã‚’æ—¥æ•°ã«å¤‰æ›ï¼‰
            T_steps = signature['observation_interval']
            T_days = T_steps  # 1ã‚¹ãƒ†ãƒƒãƒ— = 1æ—¥ã¨ä»®å®š
            T_years = T_days / 365.25
            
            # 3. è³ªé‡æ¨å®šï¼ˆæ§‹é€ çš„å½±éŸ¿åŠ›ã‹ã‚‰ï¼‰
            # å½±éŸ¿åŠ›ã¯è·é›¢ã®2ä¹—ã«åæ¯”ä¾‹ã™ã‚‹æ§‹é€ çš„çµåˆã¨ã—ã¦è§£é‡ˆ
            structural_influence = signature.get('structural_influence', 1.0)
            
            # æ‘‚å‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰è³ªé‡ã‚’é€†ç®—
            # æœ€æ¥è¿‘è·é›¢ã§ã®æ§‹é€ çš„çµåˆå¼·åº¦
            r_closest = abs(a_estimated - central_scale)
            if r_closest < 0.1:  # å†…å´ã®è»Œé“
                r_closest = a_estimated
            
            # æ§‹é€ çš„åŠ é€Ÿåº¦ï¼ˆè¦³æ¸¬ã‚¹ãƒ†ãƒƒãƒ—ã§ã®å¤‰åŒ–ç‡ï¼‰
            influence_window = T_steps * 0.1  # å½±éŸ¿æœŸé–“ã¯å‘¨æœŸã®10%
            structural_acceleration = 2 * max_deviation / (influence_window**2)
            
            # è³ªé‡ç›¸å½“å€¤ï¼ˆæ§‹é€ çš„çµåˆã‹ã‚‰ï¼‰
            # M âˆ a Ã— rÂ² ï¼ˆé‡åŠ›å®šæ•°ãªã—ã®ç›¸å¯¾å€¤ï¼‰
            mass_structural = structural_acceleration * r_closest**2
            
            # åœ°çƒè³ªé‡å˜ä½ã¸ã®å¤‰æ›ï¼ˆçµŒé¨“çš„è¼ƒæ­£å€¤ï¼‰
            # ã“ã‚Œã¯è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã¨ã®æ¯”è¼ƒã‹ã‚‰å°å‡ºã•ã‚Œã‚‹
            calibration_factor = 1e6  # æ§‹é€ å˜ä½â†’åœ°çƒè³ªé‡
            mass_earth = mass_structural * calibration_factor
            
            # 4. é›¢å¿ƒç‡ã®æ¨å®šï¼ˆæ§‹é€ ã®éå¯¾ç§°æ€§ã‹ã‚‰ï¼‰
            eccentricity = self._estimate_eccentricity_from_structure(
                structures, T_steps, signature
            )
            
            # 5. æ§‹é€ çš„ç¢ºä¿¡åº¦ã‹ã‚‰èª¤å·®ã‚’æ¨å®š
            confidence = signature['topological_confidence']
            mass_uncertainty = mass_earth * (1 - confidence)
            
            physical_params[name] = {
                'mass_earth': mass_earth,
                'mass_uncertainty': mass_uncertainty,
                'semi_major_axis_au': a_estimated,
                'period_days': T_days,
                'period_years': T_years,
                'eccentricity': eccentricity,
                'closest_approach_au': r_closest,
                'structural_confidence': confidence,
                'detection_method': signature['pattern_type']
            }
            
            if self.verbose:
                print(f"     Mass: {mass_earth:.1f} Â± {mass_uncertainty:.1f} Earth masses")
                print(f"     Semi-major axis: {a_estimated:.2f} AU")
                print(f"     Period: {T_years:.1f} years ({T_days:.0f} days)")
                print(f"     Eccentricity: {eccentricity:.2f}")
                print(f"     Confidence: {confidence:.2f}")
        
        return physical_params

    def _estimate_eccentricity_from_structure(self, structures: Dict[str, np.ndarray],
                                            period_steps: float,
                                            signature: Dict) -> float:
        """
        æ§‹é€ ã®éå¯¾ç§°æ€§ã‹ã‚‰é›¢å¿ƒç‡ã‚’æ¨å®š
        """
        # å‘¨æœŸã®åŠåˆ†ã§æ§‹é€ ã‚’åˆ†å‰²
        half_period = int(period_steps / 2)
        
        lambda_F_mag = structures['lambda_F_mag']
        
        # å„åŠå‘¨æœŸã§ã®æœ€å¤§å€¤ã‚’æ¯”è¼ƒ
        asymmetry_ratios = []
        
        for start in range(0, len(lambda_F_mag) - int(period_steps), int(period_steps)):
            first_half = lambda_F_mag[start:start+half_period]
            second_half = lambda_F_mag[start+half_period:start+int(period_steps)]
            
            if len(first_half) > 0 and len(second_half) > 0:
                max_first = np.max(first_half)
                max_second = np.max(second_half)
                
                if max_first > 0 and max_second > 0:
                    ratio = abs(max_first - max_second) / (max_first + max_second)
                    asymmetry_ratios.append(ratio)
        
        if asymmetry_ratios:
            # å¹³å‡éå¯¾ç§°æ€§ã‹ã‚‰é›¢å¿ƒç‡ã‚’æ¨å®š
            mean_asymmetry = np.mean(asymmetry_ratios)
            # çµŒé¨“çš„ãªå¤‰æ›ï¼ˆéå¯¾ç§°æ€§ã®ç´„30%ãŒé›¢å¿ƒç‡ã«å¯„ä¸ï¼‰
            eccentricity = min(0.5, mean_asymmetry * 0.3)
        else:
            eccentricity = 0.1  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        
        return eccentricity

    def analyze(self, data: pd.DataFrame, positions: np.ndarray) -> Dict:
        """
        Complete Pure LambdaÂ³ analysis pipeline with physical parameter estimation.
        """
        # 1-7. æ—¢å­˜ã®è§£æã‚¹ãƒ†ãƒƒãƒ—
        structures = self.compute_lambda_structures(positions)
        boundaries = self.detect_structural_boundaries(structures)
        breaks = self.detect_topological_breaks(structures)
        
        if boundaries['boundary_locations'].size > 0:
            if self.verbose:
                print("\nğŸ¯ Using structural boundaries to guide detection...")
            
            original_anomaly = breaks['combined_anomaly'].copy()
            boundary_score = boundaries['boundary_score']
            
            if len(boundary_score) < len(original_anomaly):
                padding = len(original_anomaly) - len(boundary_score)
                boundary_score = np.pad(boundary_score, (0, padding), mode='edge')
            
            for i in range(len(original_anomaly)):
                local_boundary = boundary_score[i] if i < len(boundary_score) else 0
                sensitivity = 1.0 + 3.0 * local_boundary
                breaks['combined_anomaly'][i] *= sensitivity
        
        recurrence_patterns = self.extract_topological_recurrence(structures)
        recurrence_patterns = self.filter_harmonics_in_recurrence(recurrence_patterns)
        structural_families = self.identify_structural_families(recurrence_patterns)
        
        representative_patterns = []
        for family_name, patterns in structural_families.items():
            if patterns:
                representative = max(patterns, key=lambda x: x['topological_confidence'])
                representative_patterns.append(representative)
        
        structural_signatures = self.decompose_structural_signatures(
            structures, representative_patterns
        )
        
        detected_structures = self.estimate_topological_parameters(
            structural_signatures, structures
        )
        
        detected_structures = self.merge_related_structures(detected_structures)
        
        # 8. NEW: ç‰©ç†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ¨å®šï¼
        physical_parameters = self.estimate_physical_parameters(
            structures, structural_signatures
        )
        
        # æ¤œå‡ºã•ã‚ŒãŸæ§‹é€ ã«ç‰©ç†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿½åŠ 
        for i, structure in enumerate(detected_structures):
            structure_name = structure['name']
            if structure_name in physical_parameters:
                structure.update(physical_parameters[structure_name])
        
        # çµæœã‚’ä¿å­˜
        self.data = data
        self.positions = positions
        self.structures = structures
        self.boundaries = boundaries
        self.breaks = breaks
        self.recurrence_patterns = recurrence_patterns
        self.structural_signatures = structural_signatures
        self.detected_structures = detected_structures
        self.physical_parameters = physical_parameters
        
        return {
            'n_structures_detected': len(detected_structures),
            'hidden_structures': detected_structures,
            'topological_patterns': structures,
            'topological_breaks': breaks,
            'structural_boundaries': boundaries,
            'physical_parameters': physical_parameters,
            'observation_steps': len(positions)
        }

    def print_results(self, expected_data: Optional[Dict] = None):
        """
        è¦³æ¸¬ã‚¹ãƒ†ãƒƒãƒ—ãƒ™ãƒ¼ã‚¹ã§çµæœã‚’è¡¨ç¤º
        
        Args:
            expected_data: æœŸå¾…ã•ã‚Œã‚‹æ§‹é€ ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
                          æŒ‡å®šã—ãªã„å ´åˆã¯æ±ç”¨çš„ãªè¡¨ç¤º
        """
        print("\n" + "="*70)
        print("ğŸŒŒ Pure LambdaÂ³ Topological Analysis Results")
        print("="*70)
        print("\nâš¡ NO TIME, NO PHYSICS, ONLY STRUCTURE!")
        print(f"ğŸ“Š Total observation steps: {len(self.positions)}")
        
        # ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬çµ±è¨ˆ
        data_range = np.max(self.positions) - np.min(self.positions)
        data_mean = np.mean(np.linalg.norm(self.positions, axis=1))
        
        print(f"\nğŸ“ˆ Data characteristics:")
        print(f"   Data range: {data_range:.3f}")
        print(f"   Mean scale: {data_mean:.3f}")
        
        # æœŸå¾…å€¤ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã®ã¿è¡¨ç¤º
        if expected_data:
            print("\nğŸ“ Reference: Expected structures")
            for name, params in expected_data.items():
                if 'mass' in params:
                    mass_earth = params['mass'] * 333000  # å¤ªé™½è³ªé‡â†’åœ°çƒè³ªé‡
                    print(f"   - {name}: period={params['period']} steps, "
                          f"a={params.get('a', 'N/A')} AU, Mâ‰ˆ{mass_earth:.1f} Earth masses")
                else:
                    print(f"   - {name}: period={params['period']} steps")
        
        # æ§‹é€ çš„å¢ƒç•Œ
        if hasattr(self, 'boundaries') and self.boundaries['boundary_locations'].size > 0:
            print(f"\nğŸŒŸ Structural Boundaries: {len(self.boundaries['boundary_locations'])}")
            print("   (Natural topological limits in observation sequence)")
            
            if len(self.boundaries['boundary_locations']) > 1:
                boundary_intervals = np.diff(self.boundaries['boundary_locations'])
                print(f"   Average interval: {np.mean(boundary_intervals):.0f} Â± "
                      f"{np.std(boundary_intervals):.0f} steps")
        
        # æ¤œå‡ºã•ã‚ŒãŸæ§‹é€ 
        print(f"\nğŸ” Detected {len(self.detected_structures)} hidden structures:")
        print("-"*70)
        
        matched_count = 0
        
        for i, structure in enumerate(self.detected_structures):
            print(f"\n{structure['name']}:")
            print(f"  Observation interval: {structure['observation_interval']:.0f} steps")
            
            # æ™‚é–“ã‚¹ã‚±ãƒ¼ãƒ«ã®è§£é‡ˆï¼ˆæ±ç”¨ï¼‰
            print(f"  Time interpretations:")
            print(f"    - As days: {structure['observation_interval']/365.25:.2f} years")
            print(f"    - As hours: {structure['observation_interval']/24:.1f} days")
            
            print(f"  Hierarchy factor: {structure['hierarchy_factor']:.2f}")
            print(f"  Topological radius: {structure['topological_radius']:.2f}")
            print(f"  Structural influence: {structure['structural_influence']:.0f}")
            
            # ç‰©ç†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆã‚‚ã—ã‚ã‚Œã°ï¼‰
            if 'mass_earth' in structure:
                print(f"\n  ğŸ“Š PHYSICAL PARAMETERS (from pure topology):")
                print(f"     Mass: {structure['mass_earth']:.1f} Â± {structure['mass_uncertainty']:.1f} Earth masses")
                print(f"     Semi-major axis: {structure['semi_major_axis_au']:.2f} AU")
                print(f"     Period: {structure['period_years']:.1f} years")
                print(f"     Eccentricity: {structure['eccentricity']:.2f}")
            
            print(f"  Detection confidence: {structure['topological_confidence']:.3f}")
            print(f"  Pattern type: {structure['pattern_type']}")
            
            # æœŸå¾…å€¤ã¨ã®ãƒãƒƒãƒãƒ³ã‚°ï¼ˆã‚‚ã—ã‚ã‚Œã°ï¼‰
            if expected_data:
                best_match = None
                best_diff = float('inf')
                
                for exp_name, params in expected_data.items():
                    diff = abs(structure['observation_interval'] - params['period']) / params['period']
                    if diff < best_diff and diff < 0.15:
                        best_diff = diff
                        best_match = exp_name
                
                if best_match:
                    matched_count += 1
                    print(f"\n  âœ… MATCHED: {best_match} "
                          f"(period diff: {best_diff*100:.1f}%)")
        
        # ã‚µãƒãƒªãƒ¼
        print("\n" + "="*70)
        print("ğŸ“Š ANALYSIS SUMMARY:")
        print(f"   Total structures detected: {len(self.detected_structures)}")
        
        if expected_data and matched_count > 0:
            print(f"   Matched with expected: {matched_count}/{len(expected_data)}")
        
        # æ§‹é€ é–“ã®é–¢ä¿‚
        if len(self.detected_structures) > 1:
            print("\nğŸ”— Structural relationships:")
            sorted_structures = sorted(self.detected_structures, 
                                     key=lambda x: x['observation_interval'])
            base_period = sorted_structures[0]['observation_interval']
            
            for s in sorted_structures[1:]:
                ratio = s['observation_interval'] / base_period
                print(f"   {s['name']} / {sorted_structures[0]['name']} = {ratio:.2f}")
        
        print("\nğŸ¯ LambdaÂ³ SUCCESS: Hidden structures revealed through pure topology!")
        print("   Transaction, not time. Structure, not physics!")
        print("="*70)

    def plot_results(self, save_path: Optional[str] = None):
        """Visualization of Pure LambdaÂ³ analysis - observation step based"""
        import matplotlib.pyplot as plt

        fig = plt.figure(figsize=(18, 14))

        # 1. è¦³æ¸¬è»Œè·¡
        ax1 = plt.subplot(3, 4, 1)
        ax1.plot(self.positions[:, 0], self.positions[:, 1],
                'k-', linewidth=0.5, alpha=0.7)
        ax1.scatter(0, 0, color='orange', s=200, marker='*', label='Center')
        ax1.set_xlabel('X [structural units]')
        ax1.set_ylabel('Y [structural units]')
        ax1.set_title('Observation Trajectory')
        ax1.axis('equal')
        ax1.grid(True, alpha=0.3)

        # 2. ãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«ãƒãƒ£ãƒ¼ã‚¸ã®ç´¯ç©
        ax2 = plt.subplot(3, 4, 2)
        steps = np.arange(len(self.structures['Q_cumulative']))
        ax2.plot(steps, self.structures['Q_cumulative'], 'b-', linewidth=2)
        ax2.set_xlabel('Observation Steps')
        ax2.set_ylabel('Q_Î› (cumulative)')
        ax2.set_title('Topological Winding')
        ax2.grid(True, alpha=0.3)

        # 3. è¤‡åˆç•°å¸¸ã‚¹ã‚³ã‚¢
        ax3 = plt.subplot(3, 4, 3)
        anomaly_steps = np.arange(len(self.breaks['combined_anomaly']))
        ax3.plot(anomaly_steps, self.breaks['combined_anomaly'], 'r-', alpha=0.7)
        ax3.set_xlabel('Observation Steps')
        ax3.set_ylabel('Anomaly Score')
        ax3.set_title('Topological Breaks')
        ax3.grid(True, alpha=0.3)

        # 4. æ§‹é€ çš„å¢ƒç•Œ
        ax4 = plt.subplot(3, 4, 4)
        boundary_steps = np.arange(len(self.boundaries['boundary_score']))
        ax4.plot(boundary_steps, self.boundaries['boundary_score'], 'purple', alpha=0.7)
        for boundary in self.boundaries['boundary_locations']:
            ax4.axvline(boundary, color='red', linestyle='--', alpha=0.5)
        ax4.set_xlabel('Observation Steps')
        ax4.set_ylabel('Boundary Score')
        ax4.set_title('Structural Boundaries')
        ax4.grid(True, alpha=0.3)

        # 5. Î›F magnitude
        ax5 = plt.subplot(3, 4, 5)
        lf_steps = np.arange(len(self.structures['lambda_F_mag']))
        ax5.plot(lf_steps, self.structures['lambda_F_mag'], 'g-', alpha=0.7)
        ax5.set_xlabel('Observation Steps')
        ax5.set_ylabel('|Î›F|')
        ax5.set_title('Structural Flow Magnitude')
        ax5.grid(True, alpha=0.3)

        # 6. ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å ´
        ax6 = plt.subplot(3, 4, 6)
        rho_steps = np.arange(len(self.structures['rho_T']))
        ax6.plot(rho_steps, self.structures['rho_T'], 'm-', alpha=0.7)
        ax6.set_xlabel('Observation Steps')
        ax6.set_ylabel('ÏT')
        ax6.set_title('Structural Tension Field')
        ax6.grid(True, alpha=0.3)

        # 7. æ¤œå‡ºã•ã‚ŒãŸå†å¸°ãƒ‘ã‚¿ãƒ¼ãƒ³
        ax7 = plt.subplot(3, 4, 7)
        if hasattr(self, 'recurrence_patterns') and self.recurrence_patterns:
            intervals = [p['observation_interval'] for p in self.recurrence_patterns[:10]]
            confidences = [p['topological_confidence'] for p in self.recurrence_patterns[:10]]
            y_pos = np.arange(len(intervals))

            ax7.barh(y_pos, intervals, color='cyan', alpha=0.7)
            ax7.set_yticks(y_pos)
            ax7.set_yticklabels([f"Pattern {i+1}" for i in range(len(intervals))])
            ax7.set_xlabel('Recurrence Interval [steps]')
            ax7.set_title('Detected Recurrence Patterns')

            # Confidence as text
            for i, (interval, conf) in enumerate(zip(intervals, confidences)):
                ax7.text(interval + 20, i, f'{conf:.1f}', va='center')

        # 8. Q_residual
        ax8 = plt.subplot(3, 4, 8)
        q_steps = np.arange(len(self.breaks['Q_residual']))
        ax8.plot(q_steps, self.breaks['Q_residual'], 'c-', alpha=0.7)
        ax8.set_xlabel('Observation Steps')
        ax8.set_ylabel('Q_Î› Residual')
        ax8.set_title('Topological Charge Anomaly')
        ax8.grid(True, alpha=0.3)

        # 9. æ¤œå‡ºçµæœã‚µãƒãƒªãƒ¼
        ax9 = plt.subplot(3, 4, 9)
        ax9.axis('off')

        summary = "ğŸŒŸ Pure LambdaÂ³ Detection Results\n" + "="*40 + "\n\n"
        summary += "NO TIME. NO PHYSICS. ONLY STRUCTURE.\n\n"
        summary += f"Total observation steps: {len(self.positions)}\n"
        summary += f"Structural boundaries: {len(self.boundaries['boundary_locations'])}\n"
        summary += f"Detected structures: {len(self.detected_structures)}\n\n"

        for structure in self.detected_structures[:3]:
            summary += f"{structure['name']}:\n"
            summary += f"  Interval: {structure['observation_interval']:.0f} steps\n"
            summary += f"  Confidence: {structure['topological_confidence']:.1f}\n\n"

        ax9.text(0.1, 0.9, summary, transform=ax9.transAxes,
                fontsize=10, verticalalignment='top', fontfamily='monospace')

        # 10. ãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«æ§‹é€ å›³
        ax10 = plt.subplot(3, 4, 10)
        theta = np.linspace(0, 2*np.pi, 100)

        # ä¸»æ§‹é€ 
        r_primary = np.mean(np.linalg.norm(self.positions, axis=1))
        ax10.plot(r_primary * np.cos(theta), r_primary * np.sin(theta),
                'k--', alpha=0.5, label='Primary')

        # æ¤œå‡ºã•ã‚ŒãŸæ§‹é€ 
        colors = ['r', 'g', 'b', 'c', 'm', 'y']
        for i, structure in enumerate(self.detected_structures[:5]):
            r = structure['topological_radius']
            ax10.plot(r * np.cos(theta), r * np.sin(theta),
                    color=colors[i % len(colors)], linestyle='--', alpha=0.5,
                    label=f"{structure['name']} ({structure['observation_interval']:.0f})")

        ax10.scatter(0, 0, color='orange', s=200, marker='*')
        ax10.set_xlabel('X [structural units]')
        ax10.set_ylabel('Y [structural units]')
        ax10.set_title('Topological Architecture')
        ax10.legend(fontsize=8)
        ax10.axis('equal')
        ax10.grid(True, alpha=0.3)

        # 11. ãƒ˜ãƒªã‚·ãƒ†ã‚£
        ax11 = plt.subplot(3, 4, 11)
        hel_steps = np.arange(len(self.structures['helicity']))
        ax11.plot(hel_steps, self.structures['helicity'], 'y-', alpha=0.7)
        ax11.set_xlabel('Observation Steps')
        ax11.set_ylabel('Helicity')
        ax11.set_title('Structural Helicity')
        ax11.grid(True, alpha=0.3)

        # 12. ä½ç›¸ç©ºé–“ï¼ˆç•°å¸¸åº¦ã§è‰²ä»˜ã‘ï¼‰
        ax12 = plt.subplot(3, 4, 12)
        n_points = min(len(self.positions)-1,
                      len(self.structures['lambda_F']),
                      len(self.breaks['combined_anomaly']))

        scatter = ax12.scatter(self.positions[:n_points, 0],
                            self.structures['lambda_F'][:n_points, 0],
                            c=self.breaks['combined_anomaly'][:n_points],
                            cmap='plasma', s=1, alpha=0.7)
        plt.colorbar(scatter, ax=ax12, label='Anomaly')
        ax12.set_xlabel('X [structural units]')
        ax12.set_ylabel('Î›F_x [Î”structure/step]')
        ax12.set_title('Phase Space (colored by anomaly)')
        ax12.grid(True, alpha=0.3)

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            if self.verbose:
                print(f"\nğŸ“Š Figure saved to {save_path}")

        plt.show()


def export_results(analyzer, input_filename):
    """Export analysis results to files"""
    import json
    from datetime import datetime

    # Create output filename
    base_name = input_filename.replace('.csv', '')
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

    # Export detected structures
    structures_data = []
    for structure in analyzer.detected_structures:
        structure_dict = {
            'name': structure['name'],
            'observation_interval_steps': float(structure['observation_interval']),
            'hierarchy_factor': float(structure['hierarchy_factor']),
            'topological_radius': float(structure['topological_radius']),
            'structural_influence': float(structure['structural_influence']),
            'confidence': float(structure['topological_confidence']),
            'pattern_type': structure['pattern_type']
        }
        
        # Add physical parameters if available
        if 'mass_earth' in structure:
            structure_dict.update({
                'mass_earth': float(structure['mass_earth']),
                'mass_uncertainty': float(structure['mass_uncertainty']),
                'semi_major_axis_au': float(structure['semi_major_axis_au']),
                'period_days': float(structure['period_days']),
                'period_years': float(structure['period_years']),
                'eccentricity': float(structure['eccentricity']),
                'closest_approach_au': float(structure['closest_approach_au'])
            })
        
        structures_data.append(structure_dict)

    output_data = {
        'analysis_type': 'Pure LambdaÂ³ Topological Analysis',
        'observation_steps': len(analyzer.positions),
        'structures_detected': len(structures_data),
        'structural_boundaries': len(analyzer.boundaries['boundary_locations']),
        'primary_recurrence_interval': float(analyzer.detect_primary_recurrence(analyzer.structures)),
        'detected_structures': structures_data,
        'metadata': {
            'input_file': input_filename,
            'analysis_timestamp': timestamp,
            'lambda3_version': '2.0.0-physics'
        }
    }

    # Save JSON
    output_file = f"{base_name}_lambda3_results_{timestamp}.json"
    with open(output_file, 'w') as f:
        json.dump(output_data, f, indent=2)

    print(f"\nğŸ’¾ Results exported to: {output_file}")


def main():
    """Main execution function - Pure LambdaÂ³ with multifocus capability!"""
    parser = argparse.ArgumentParser(
        description='Pure LambdaÂ³ Framework - Topological Structure Detection from Observation Sequence'
    )
    parser.add_argument(
        '--data',
        type=str,
        default='challenge_blackhole_alpha_noisy.csv',
        help='Path to CSV file containing observation sequence'
    )
    parser.add_argument(
        '--save-plot',
        type=str,
        default=None,
        help='Path to save analysis plot'
    )
    parser.add_argument(
        '--quiet',
        action='store_true',
        help='Suppress verbose output'
    )
    parser.add_argument(
        '--max-structures',
        type=int,
        default=5,
        help='Maximum number of structures to detect (default: 5)'
    )

    args = parser.parse_args()

    print("\nâœ¨ Pure LambdaÂ³ Analysis - Transaction-based Reality âœ¨")
    print("=" * 60)
    print("NO TIME. NO PHYSICS. ONLY STRUCTURE.")
    print("=" * 60)

    # Initialize analyzer
    analyzer = PureLambda3Analyzer(verbose=not args.quiet)

    # Load observation sequence
    data, positions = analyzer.load_and_clean_data(args.data)

    # Validate observation sequence
    n_observations = len(positions)
    print(f"\nğŸ“Š Observation sequence loaded:")
    print(f"   Total steps: {n_observations}")
    print(f"   Missing data interpolated: âœ“")

    if n_observations < 500:
        print("\nâš ï¸  Warning: Short observation sequence detected!")
        print(f"   Recommended: >500 steps, Got: {n_observations} steps")
        print("   Results may be less reliable.")

    # Run pure topological analysis
    print("\nğŸŒŒ Starting LambdaÂ³ analysis...")

    # ãƒãƒ«ãƒãƒ•ã‚©ãƒ¼ã‚«ã‚¹åˆ¤å®š
    if n_observations >= 2500:
        print("\nğŸ”­ MULTI-FOCUS MODE ACTIVATED!")
        print("   Long observation sequence detected.")
        print("   Running dual-scale analysis for complete structure detection...")

        # Phase 1: è¿‘è·é›¢æ¢æŸ»ï¼ˆçŸ­å‘¨æœŸæ¤œå‡ºç”¨ï¼‰
        print("\n" + "="*60)
        print("ğŸ“¡ PHASE 1: Near-field Detection (1500 steps)")
        print("="*60)
        analyzer_near = PureLambda3Analyzer(verbose=not args.quiet)
        results_near = analyzer_near.analyze(data.iloc[:1500], positions[:1500])

        print(f"\nâœ… Phase 1 complete: {results_near['n_structures_detected']} structures detected")

        # Phase 2: é è·é›¢æ¢æŸ»ï¼ˆé•·å‘¨æœŸæ¤œå‡ºç”¨ï¼‰
        print("\n" + "="*60)
        print("ğŸ“¡ PHASE 2: Far-field Detection (full data)")
        print("="*60)
        results_far = analyzer.analyze(data, positions)

        print(f"\nâœ… Phase 2 complete: {results_far['n_structures_detected']} structures detected")

        # çµæœçµ±åˆã‚µãƒãƒªãƒ¼
        print("\n" + "="*60)
        print("ğŸŒŸ MULTI-FOCUS DETECTION SUMMARY")
        print("="*60)

        # è¿‘è·é›¢ã§æ¤œå‡ºã—ãŸæ§‹é€ ã‚’è¡¨ç¤º
        if results_near['n_structures_detected'] > 0:
            print("\nğŸ“ Near-field structures (likely X, Y):")
            for s in analyzer_near.detected_structures:
                print(f"   - {s['name']}: {s['observation_interval']:.0f} steps "
                      f"(confidence: {s['topological_confidence']:.2f})")

        # é è·é›¢ã§æ¤œå‡ºã—ãŸæ§‹é€ ã‚’è¡¨ç¤º
        if results_far['n_structures_detected'] > 0:
            print("\nğŸ“ Far-field structures (likely Z):")
            for s in analyzer.detected_structures:
                print(f"   - {s['name']}: {s['observation_interval']:.0f} steps "
                      f"(confidence: {s['topological_confidence']:.2f})")

        # åˆè¨ˆ
        total_unique = results_near['n_structures_detected'] + results_far['n_structures_detected']
        print(f"\nğŸ“Š Total structures detected: {total_unique}")
        print("   (Note: Some structures may be detected in both phases)")

        # ãƒ¡ã‚¤ãƒ³ã®çµæœã¯é è·é›¢ã‚’ä½¿ç”¨ï¼ˆãƒ—ãƒ­ãƒƒãƒˆç”¨ï¼‰
        results = results_far

    else:
        # é€šå¸¸ã®å˜ä¸€ã‚¹ã‚±ãƒ¼ãƒ«è§£æ
        results = analyzer.analyze(data, positions)

    # Print detailed results
    print("\n" + "="*60)
    print("ğŸ“‹ DETAILED ANALYSIS RESULTS")
    print("="*60)
    analyzer.print_results()

    # Additional summary
    print("\nğŸ“ˆ Analysis Summary:")
    print(f"   Observation steps analyzed: {results['observation_steps']}")
    print(f"   Structures detected: {results['n_structures_detected']}")
    print(f"   Structural boundaries found: {len(results['structural_boundaries']['boundary_locations'])}")

    # Plot results
    if not args.quiet:
        print("\nğŸ“Š Generating visualization...")
        analyzer.plot_results(save_path=args.save_plot)

    # Export results
    export_results(analyzer, args.data)

    print("\nâœ¨ LambdaÂ³ analysis complete!")
    print("   The hidden structure has been revealed through pure topology!")
    print("   Remember: Time is an illusion. Only Transaction exists! ğŸŒ€")


# For Jupyter/Colab compatibility
if __name__ == "__main__":
    import sys

    # Check if running in notebook
    try:
        get_ipython()
        in_notebook = True
    except NameError:
        in_notebook = False

    if in_notebook:
        print("ğŸŒŸ Running in notebook mode...")
        print("   Creating analyzer with default parameters")

        # Notebook-friendly initialization
        analyzer = PureLambda3Analyzer(verbose=True)

        # Check for data file
        import os
        default_file = 'challenge_blackhole_alpha_noisy1500.csv'

        if os.path.exists(default_file):
            print(f"\nğŸ“Š Loading {default_file}...")
            data, positions = analyzer.load_and_clean_data(default_file)

            print("\nğŸŒŒ Running analysis...")
            results = analyzer.analyze(data, positions)

            analyzer.print_results()
            analyzer.plot_results()
        else:
            print("\nâš ï¸ Data file not found!")
            print(f"   Please upload: {default_file}")
            print("   Or generate test data first.")

    else:
        # Standard command-line execution
        try:
            main()
        except KeyboardInterrupt:
            print("\n\nâš¡ Analysis interrupted by user")
            sys.exit(0)
        except Exception as e:
            print(f"\nâŒ Error occurred: {str(e)}")
            import traceback
            traceback.print_exc()
            sys.exit(1)

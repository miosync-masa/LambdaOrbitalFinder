
"""
Pure LambdaÂ³ Framework - Topological Structure Analysis
NO TIME, NO PHYSICS, ONLY STRUCTURE!

This revolutionary approach detects hidden structures using only topological 
properties of observation sequences - no physical constants required.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.optimize import curve_fit, minimize
from scipy.signal import find_peaks, correlate, hilbert, savgol_filter
from scipy.interpolate import interp1d
from scipy.fft import fft, fftfreq
from scipy.ndimage import gaussian_filter1d, median_filter
import argparse
from typing import Tuple, Dict, List, Optional
import warnings
warnings.filterwarnings('ignore')


class PureLambda3Analyzer:
    """
    Pure LambdaÂ³ Framework - Topological Structure Analysis
    
    COMPLETELY PHYSICS-FREE VERSION!
    No G, no masses, no Kepler's laws - just pure structure!
    """
    
    # Structural constants (no physical meaning)
    STRUCTURAL_RECURRENCE_FACTORS = [0.5, 0.67, 0.75, 1.0, 1.33, 1.5, 2.0]
    TOPOLOGICAL_COHERENCE_THRESHOLD = 0.15
    
    def __init__(self, verbose: bool = True):
        """Initialize the Pure LambdaÂ³ Analyzer."""
        self.verbose = verbose
        self.results = {}
        self.adaptive_params = None
        
    def load_and_clean_data(self, filename: str) -> Tuple[pd.DataFrame, np.ndarray]:
        """Load and clean observational data."""
        if self.verbose:
            print(f"ğŸ“Š Loading observational data from {filename}...")
            
        df = pd.read_csv(filename, index_col='step')
        
        # Count missing values
        missing_x = df['x_noisy'].isna().sum()
        missing_y = df['y_noisy'].isna().sum()
        total_points = len(df)
        
        if self.verbose:
            print(f"   Data points: {total_points}")
            print(f"   Missing: X={missing_x} ({missing_x/total_points*100:.1f}%), " +
                  f"Y={missing_y} ({missing_y/total_points*100:.1f}%)")
        
        # Interpolate missing values with cubic splines
        df['x_clean'] = df['x_noisy'].interpolate(method='cubic', limit_direction='both')
        df['y_clean'] = df['y_noisy'].interpolate(method='cubic', limit_direction='both')
        df['z_clean'] = df['z'].fillna(0)
        
        # Extract clean positions
        positions = df[['x_clean', 'y_clean', 'z_clean']].values
        
        return df, positions

    def compute_lambda_structures(self, positions: np.ndarray) -> Dict[str, np.ndarray]:
        """
        Compute fundamental LambdaÂ³ structural quantities from observation sequence.
        è¦³æ¸¬ã‚¹ãƒ†ãƒƒãƒ—åˆ—ã‹ã‚‰åŸºæœ¬çš„ãªÎ›æ§‹é€ ã‚’è¨ˆç®—
        """
        if self.verbose:
            print("\nğŸŒŒ Computing LambdaÂ³ structural tensors from observation steps...")
        
        n_steps = len(positions)
        
        # 1. Î›F - Structural flow field (è¦³æ¸¬ã‚¹ãƒ†ãƒƒãƒ—é–“ã®æ§‹é€ å¤‰åŒ–)
        lambda_F = np.zeros((n_steps-1, 3))
        lambda_F_mag = np.zeros(n_steps-1)
        
        for step in range(n_steps-1):
            lambda_F[step] = positions[step+1] - positions[step]
            lambda_F_mag[step] = np.linalg.norm(lambda_F[step])
        
        # 2. Î›FF - Second-order structure (æ§‹é€ å¤‰åŒ–ã®å¤‰åŒ–)
        lambda_FF = np.zeros((n_steps-2, 3))
        lambda_FF_mag = np.zeros(n_steps-2)
        
        for step in range(n_steps-2):
            lambda_FF[step] = lambda_F[step+1] - lambda_F[step]
            lambda_FF_mag[step] = np.linalg.norm(lambda_FF[step])
        
        # 3. ÏT - Tension field (å±€æ‰€çš„ãªæ§‹é€ ã®å¼µåŠ›)
        window_steps = max(3, n_steps // 200)  # è¦³æ¸¬ã‚¹ãƒ†ãƒƒãƒ—ã®0.5%
        rho_T = np.zeros(n_steps)
        
        for step in range(n_steps):
            start_step = max(0, step - window_steps)
            end_step = min(n_steps, step + window_steps + 1)
            local_positions = positions[start_step:end_step]
            
            if len(local_positions) > 1:
                centered = local_positions - np.mean(local_positions, axis=0)
                cov = np.cov(centered.T)
                rho_T[step] = np.trace(cov)
        
        # 4. Q_Î› - Topological charge (ä½ç›¸çš„å·»ãæ•°ã®å¤‰åŒ–)
        Q_lambda = np.zeros(n_steps-1)
        
        for step in range(1, n_steps-1):
            if lambda_F_mag[step] > 1e-10 and lambda_F_mag[step-1] > 1e-10:
                v1 = lambda_F[step-1] / lambda_F_mag[step-1]
                v2 = lambda_F[step] / lambda_F_mag[step]
                
                cos_angle = np.clip(np.dot(v1, v2), -1, 1)
                angle = np.arccos(cos_angle)
                
                # 2Då¹³é¢ã§ã®å›è»¢æ–¹å‘
                cross_z = v1[0]*v2[1] - v1[1]*v2[0]
                signed_angle = angle if cross_z >= 0 else -angle
                
                Q_lambda[step] = signed_angle / (2 * np.pi)
        
        # 5. Helicity (æ§‹é€ ã®ã­ã˜ã‚Œ)
        helicity = np.zeros(n_steps-1)
        for step in range(n_steps-1):
            if step > 0:
                r = positions[step]
                v = lambda_F[step-1]
                if np.linalg.norm(r) > 0 and np.linalg.norm(v) > 0:
                    helicity[step] = np.dot(r, v) / (np.linalg.norm(r) * np.linalg.norm(v))
        
        if self.verbose:
            print(f"   Computed tensors for {n_steps} observation steps")
            print(f"   Î›F dimension: {lambda_F.shape}")
            print(f"   Q_Î› total winding: {np.sum(Q_lambda):.3f}")
            print(f"   Mean tension ÏT: {np.mean(rho_T):.3f}")
        
        return {
            'lambda_F': lambda_F,
            'lambda_F_mag': lambda_F_mag,
            'lambda_FF': lambda_FF,
            'lambda_FF_mag': lambda_FF_mag,
            'rho_T': rho_T,
            'Q_lambda': Q_lambda,
            'Q_cumulative': np.cumsum(Q_lambda),
            'helicity': helicity,
            'positions': positions,
            'n_observation_steps': n_steps
        }

    def detect_topological_breaks(self, structures: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:
        """
        Detect topological breaks and anomalies in observation sequence
        è¦³æ¸¬ã‚¹ãƒ†ãƒƒãƒ—åˆ—ã«ãŠã‘ã‚‹ãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«ãªç ´ã‚Œã¨ç•°å¸¸ã‚’æ¤œå‡º
        """
        Q_cumulative = structures['Q_cumulative']
        lambda_F_mag = structures['lambda_F_mag']
        lambda_FF_mag = structures['lambda_FF_mag']
        rho_T = structures['rho_T']
        n_steps = structures['n_observation_steps']
        
        # è¦³æ¸¬çª“ã®ã‚µã‚¤ã‚ºï¼ˆã‚¹ãƒ†ãƒƒãƒ—æ•°ã®1%ï¼‰
        window_steps = max(5, n_steps // 100)
        
        if self.verbose:
            print("\nğŸ” Detecting topological breaks in observation sequence...")
            print(f"   Total observation steps: {n_steps}")
            print(f"   Analysis window: {window_steps} steps")
        
        # 1. ãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«ãƒãƒ£ãƒ¼ã‚¸ã®ç ´ã‚Œ
        if len(Q_cumulative) > 20:
            # Savitzky-Golayãƒ•ã‚£ãƒ«ã‚¿ã§æ»‘ã‚‰ã‹ãªæˆåˆ†ã‚’æŠ½å‡º
            window_length = min(15, len(Q_cumulative)//15*2+1)
            if window_length % 2 == 0:
                window_length += 1
            
            from scipy.signal import savgol_filter
            Q_smooth = savgol_filter(Q_cumulative, 
                                  window_length=window_length, 
                                  polyorder=3)
            Q_residual = Q_cumulative - Q_smooth
        else:
            Q_residual = Q_cumulative - np.mean(Q_cumulative)
        
        # 2. æ§‹é€ ãƒ•ãƒ­ãƒ¼ï¼ˆÎ›Fï¼‰ã®ç•°å¸¸æ¤œå‡º
        lambda_F_anomaly = np.zeros_like(lambda_F_mag)
        for step in range(len(lambda_F_mag)):
            start = max(0, step - window_steps)
            end = min(len(lambda_F_mag), step + window_steps + 1)
            
            local_mean = np.mean(lambda_F_mag[start:end])
            local_std = np.std(lambda_F_mag[start:end])
            
            if local_std > 0:
                # å±€æ‰€çš„ãªæ¨™æº–åŒ–
                lambda_F_anomaly[step] = (lambda_F_mag[step] - local_mean) / local_std
        
        # 3. æ§‹é€ åŠ é€Ÿåº¦ï¼ˆÎ›FFï¼‰ã®ç•°å¸¸
        accel_window = max(3, window_steps // 2)
        lambda_FF_anomaly = np.zeros_like(lambda_FF_mag)
        
        for step in range(len(lambda_FF_mag)):
            start = max(0, step - accel_window)
            end = min(len(lambda_FF_mag), step + accel_window + 1)
            
            local_mean = np.mean(lambda_FF_mag[start:end])
            local_std = np.std(lambda_FF_mag[start:end])
            
            if local_std > 0:
                lambda_FF_anomaly[step] = (lambda_FF_mag[step] - local_mean) / local_std
        
        # 4. ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å ´ã®è·³èº
        from scipy.ndimage import gaussian_filter1d
        rho_T_smooth = gaussian_filter1d(rho_T, sigma=window_steps/3)
        rho_T_breaks = np.abs(rho_T - rho_T_smooth)
        
        # 5. è¤‡åˆçš„ãªç•°å¸¸ã‚¹ã‚³ã‚¢
        # å„æˆåˆ†ã®é•·ã•ã‚’åˆã‚ã›ã‚‹
        min_len = min(len(Q_residual), len(lambda_F_anomaly), 
                    len(lambda_FF_anomaly), len(rho_T_breaks)-1)
        
        # é‡ã¿ä»˜ãåˆæˆï¼ˆãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«ãªé‡è¦åº¦ã«åŸºã¥ãï¼‰
        combined_anomaly = (
            np.abs(Q_residual[:min_len]) * 3.0 +        # Q_Î›ã®ç ´ã‚ŒãŒæœ€é‡è¦
            np.abs(lambda_F_anomaly[:min_len]) * 1.5 +  # ãƒ•ãƒ­ãƒ¼ã®ç•°å¸¸
            np.abs(lambda_FF_anomaly[:min_len]) * 2.0 + # åŠ é€Ÿåº¦ã®ç•°å¸¸
            rho_T_breaks[:min_len] * 1.5                # ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®è·³èº
        ) / 8.0
        
        # ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã®çµ±è¨ˆ
        if self.verbose:
            n_high_anomaly = np.sum(combined_anomaly > np.mean(combined_anomaly) + 2*np.std(combined_anomaly))
            print(f"   High anomaly steps: {n_high_anomaly} ({n_high_anomaly/len(combined_anomaly)*100:.1f}%)")
            
            # å„æˆåˆ†ã®å¯„ä¸
            contributions = {
                'Q_residual': np.mean(np.abs(Q_residual[:min_len])),
                'lambda_F': np.mean(np.abs(lambda_F_anomaly[:min_len])),
                'lambda_FF': np.mean(np.abs(lambda_FF_anomaly[:min_len])),
                'rho_T': np.mean(rho_T_breaks[:min_len])
            }
            
            max_contrib = max(contributions.values())
            print("   Anomaly contributions:")
            for name, value in contributions.items():
                print(f"     {name}: {value/max_contrib*100:.0f}%")
        
        return {
            'Q_residual': Q_residual,
            'lambda_F_anomaly': lambda_F_anomaly,
            'lambda_FF_anomaly': lambda_FF_anomaly,
            'rho_T_breaks': rho_T_breaks,
            'combined_anomaly': combined_anomaly,
            'window_steps': window_steps,
            'n_observation_steps': n_steps
        }    

    def detect_structural_boundaries(self, structures: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:
        """
        Detect pure structural boundaries in observation sequence
        è¦³æ¸¬ã‚¹ãƒ†ãƒƒãƒ—åˆ—ã®ä¸­ã§æ§‹é€ çš„ãªå¢ƒç•Œã‚’æ¤œå‡ºï¼ˆç‰©ç†å®šæ•°ãªã—ï¼ï¼‰
        """
        if self.verbose:
            print("\nğŸŒŸ Detecting structural boundaries in observation sequence...")
        
        Q_cumulative = structures['Q_cumulative']
        lambda_F = structures['lambda_F']
        rho_T = structures['rho_T']
        n_steps = len(structures['positions'])
        
        # 1. Q_Î›ã®ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒè§£æ
        def compute_local_fractal_dimension(series, window_steps=30):
            """è¦³æ¸¬çª“å†…ã§ã®ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒã‚’è¨ˆç®—"""
            dims = []
            for step in range(window_steps, len(series) - window_steps):
                local = series[step-window_steps:step+window_steps]
                
                # Box-counting for 1D series
                scales = [2, 4, 8, 16]
                counts = []
                for scale in scales:
                    boxes = 0
                    for j in range(0, len(local)-scale, scale):
                        segment = local[j:j+scale]
                        if np.ptp(segment) > 0:
                            boxes += 1
                    counts.append(boxes if boxes > 0 else 1)
                
                if len(counts) > 1 and max(counts) > min(counts):
                    log_scales = np.log(scales[:len(counts)])
                    log_counts = np.log(counts)
                    slope = np.polyfit(log_scales, log_counts, 1)[0]
                    dims.append(-slope)
                else:
                    dims.append(1.0)
            
            return np.array(dims)
        
        # 2. Î›Fã®å¤šã‚¹ã‚±ãƒ¼ãƒ«æ§‹é€ çš„ä¸€è²«æ€§
        def compute_structural_coherence(lambda_F, scale_steps=[5, 10, 20, 40]):
            """ç•°ãªã‚‹è¦³æ¸¬ã‚¹ã‚±ãƒ¼ãƒ«ã§ã®æ§‹é€ ã®ä¸€è²«æ€§"""
            coherences = []
            
            for scale in scale_steps:
                if scale >= len(lambda_F):
                    continue
                    
                coherence_values = []
                for step in range(scale, len(lambda_F) - scale):
                    # éå»ã¨æœªæ¥ã®å±€æ‰€ãƒ™ã‚¯ãƒˆãƒ«
                    v_past = lambda_F[step-scale:step]
                    v_future = lambda_F[step:step+scale]
                    
                    past_mean = np.mean(v_past, axis=0)
                    future_mean = np.mean(v_future, axis=0)
                    
                    if np.linalg.norm(past_mean) > 0 and np.linalg.norm(future_mean) > 0:
                        coherence = np.dot(past_mean, future_mean) / (
                            np.linalg.norm(past_mean) * np.linalg.norm(future_mean)
                        )
                        coherence_values.append(coherence)
                
                if coherence_values:
                    coherences.append(np.array(coherence_values))
            
            return coherences
        
        # 3. ãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«çµåˆå¼·åº¦
        def compute_coupling_strength(Q_series, window_steps=50):
            """è¦³æ¸¬çª“å†…ã§ã®æ§‹é€ çš„çµåˆã®å¼·ã•"""
            n = len(Q_series)
            coupling = np.zeros(n)
            
            for step in range(window_steps, n - window_steps):
                local_Q = Q_series[step-window_steps:step+window_steps]
                
                local_var = np.var(np.diff(local_Q))
                global_var = np.var(np.diff(Q_series))
                
                if global_var > 0:
                    coupling[step] = 1 - np.abs(local_var - global_var) / global_var
                else:
                    coupling[step] = 1.0
            
            return np.clip(coupling, 0, 1)
        
        # 4. æ§‹é€ çš„ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼å‹¾é…
        def compute_structural_entropy(rho_T, window_steps=30):
            """ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å ´ã®æƒ…å ±ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼"""
            entropy = np.zeros(len(rho_T))
            
            for step in range(window_steps, len(rho_T) - window_steps):
                local_rho = rho_T[step-window_steps:step+window_steps]
                
                if np.sum(local_rho) > 0:
                    p = local_rho / np.sum(local_rho)
                    entropy[step] = -np.sum(p * np.log(p + 1e-10))
            
            return entropy
        
        # å…¨ã¦ã®æ§‹é€ çš„æŒ‡æ¨™ã‚’è¨ˆç®—
        window_steps = max(30, n_steps // 50)  # è¦³æ¸¬ã‚¹ãƒ†ãƒƒãƒ—ã®2%
        fractal_dims = compute_local_fractal_dimension(Q_cumulative, window_steps)
        coherences = compute_structural_coherence(lambda_F)
        coupling = compute_coupling_strength(Q_cumulative, window_steps)
        entropy = compute_structural_entropy(rho_T, window_steps)
        
        # æœ€å°é•·ã«æ­£è¦åŒ–
        min_len = min(len(fractal_dims), len(coupling), len(entropy))
        if coherences and len(coherences[0]) > 0:
            min_len = min(min_len, len(coherences[0]))
        
        # å„æŒ‡æ¨™ã®å‹¾é…ã‚’è¨ˆç®—
        if len(fractal_dims) > 1:
            fractal_gradient = np.abs(np.gradient(fractal_dims[:min_len]))
        else:
            fractal_gradient = np.zeros(min_len)
        
        if coherences and len(coherences[0]) > 0:
            coherence_signal = coherences[0]
            coherence_drop = 1 - coherence_signal[:min_len]
        else:
            coherence_drop = np.zeros(min_len)
        
        coupling_weakness = 1 - coupling[:min_len]
        
        if len(entropy) > 1:
            entropy_gradient = np.abs(np.gradient(entropy[:min_len]))
        else:
            entropy_gradient = np.zeros(min_len)
        
        # å¢ƒç•Œã‚¹ã‚³ã‚¢ã®åˆæˆ
        boundary_score = (
            2.0 * fractal_gradient +      # ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒã®å¤‰åŒ–
            1.5 * coherence_drop +        # æ§‹é€ çš„ä¸€è²«æ€§ã®ä½ä¸‹
            1.0 * coupling_weakness +     # çµåˆã®å¼±ã¾ã‚Š
            1.0 * entropy_gradient        # æƒ…å ±éšœå£
        ) / 5.5
        
        # å¢ƒç•Œä½ç½®ã®æ¤œå‡ºï¼ˆè¦³æ¸¬ã‚¹ãƒ†ãƒƒãƒ—å˜ä½ï¼‰
        if len(boundary_score) > 10:
            min_distance_steps = max(50, n_steps // 30)  # è¦³æ¸¬ã‚¹ãƒ†ãƒƒãƒ—ã®3.3%
            peaks, properties = find_peaks(
                boundary_score, 
                height=np.mean(boundary_score) + np.std(boundary_score),
                distance=min_distance_steps
            )
        else:
            peaks = np.array([])
        
        if self.verbose:
            print(f"   Found {len(peaks)} structural boundaries")
            if len(peaks) > 0:
                print(f"   Boundary locations (steps): {peaks[:5].tolist()}...")
                strengths = boundary_score[peaks[:5]]
                strength_str = ", ".join([f"{s:.3f}" for s in strengths])
                print(f"   Boundary strengths: [{strength_str}]...")
        
        return {
            'boundary_score': boundary_score,
            'boundary_locations': peaks,
            'fractal_dimension': fractal_dims,
            'structural_coherence': coherences[0] if coherences else np.array([]),
            'coupling_strength': coupling,
            'structural_entropy': entropy,
            'n_observation_steps': n_steps
        }

    def filter_harmonics_in_recurrence(self, recurrence_patterns: List[Dict]) -> List[Dict]:
        """
        é«˜èª¿æ³¢ã‚’é™¤å¤–ã—ã¦åŸºæœ¬çš„ãªå†å¸°ãƒ‘ã‚¿ãƒ¼ãƒ³ã ã‘ã‚’æŠ½å‡º
        """
        if not recurrence_patterns:
            return recurrence_patterns
        
        filtered = []
        used = set()
        
        # è¦³æ¸¬é–“éš”ã§ã‚½ãƒ¼ãƒˆï¼ˆé•·ã„é †ï¼‰
        sorted_patterns = sorted(recurrence_patterns, 
                              key=lambda x: x['observation_interval'], 
                              reverse=True)
        
        for i, pattern in enumerate(sorted_patterns):
            if i in used:
                continue
                
            # ã“ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åŸºæœ¬æ³¢ã¨ã—ã¦æ¡ç”¨
            filtered.append(pattern)
            used.add(i)
            
            # ã“ã®åŸºæœ¬æ³¢ã®é«˜èª¿æ³¢ã‚’é™¤å¤–
            base_interval = pattern['observation_interval']
            
            for j, other in enumerate(sorted_patterns):
                if j in used or j == i:
                    continue
                    
                # æ•´æ•°æ¯”ã‚’ãƒã‚§ãƒƒã‚¯
                ratio = base_interval / other['observation_interval']
                
                # 2, 3, 4, 5å€ã®é«˜èª¿æ³¢ãªã‚‰é™¤å¤–
                for n in [2, 3, 4, 5]:
                    if abs(ratio - n) < 0.1:
                        used.add(j)
                        if self.verbose:
                            print(f"   Filtered harmonic: {other['observation_interval']:.0f} steps "
                                  f"(n={n} of {base_interval:.0f})")
                        break
        
        return filtered            

    def extract_topological_recurrence(self, structures: Dict[str, np.ndarray]) -> List[Dict]:
        """
        Extract STRUCTURAL recurrence patterns from observation sequence
        è¦³æ¸¬ã‚¹ãƒ†ãƒƒãƒ—ã®ä¸­ã«ç¾ã‚Œã‚‹æ§‹é€ çš„å†å¸°ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º
        """
        if self.verbose:
            print("\nğŸŒŒ Extracting topological recurrence patterns from observation steps...")
        
        Q_cumulative = structures['Q_cumulative']
        lambda_F = structures['lambda_F']
        rho_T = structures['rho_T']
        n_steps = len(Q_cumulative)  # è¦³æ¸¬ã‚¹ãƒ†ãƒƒãƒ—æ•°
        
        recurrence_patterns = []
        
        # 1. Q_Î›ã®å·»ãæ•°ã«ã‚ˆã‚‹æ§‹é€ çš„å›å¸°
        Q_initial = Q_cumulative[0] if n_steps > 0 else 0
        for step_i in range(n_steps//10, n_steps):
            Q_diff = abs(Q_cumulative[step_i] - Q_initial - round(Q_cumulative[step_i] - Q_initial))
            if Q_diff < 0.1:  # ã»ã¼æ•´æ•°å·»ã
                recurrence_patterns.append({
                    'observation_interval': step_i,  # åˆæœŸã‹ã‚‰ã®è¦³æ¸¬ã‚¹ãƒ†ãƒƒãƒ—æ•°
                    'winding_number': round(Q_cumulative[step_i] - Q_initial),
                    'structural_coherence': 1.0 - Q_diff,
                    'pattern_type': 'Q_winding'
                })
        
        # 2. Î›Fãƒ™ã‚¯ãƒˆãƒ«å ´ã®ä½ç›¸ç©ºé–“å›å¸°
        for current_step in range(n_steps//4, len(lambda_F)):
            for past_step in range(min(current_step//2, len(lambda_F))):
                if current_step < len(lambda_F) and past_step < len(lambda_F):
                    # ä½ç›¸ç©ºé–“ã§ã®æ§‹é€ çš„è·é›¢
                    phase_distance = np.linalg.norm(lambda_F[current_step] - lambda_F[past_step])
                    
                    if phase_distance < np.std(structures['lambda_F_mag']) * 0.2:
                        recurrence_patterns.append({
                            'observation_interval': current_step - past_step,  # ã‚¹ãƒ†ãƒƒãƒ—é–“éš”
                            'phase_similarity': 1.0 / (1.0 + phase_distance),
                            'structural_coherence': np.exp(-phase_distance),
                            'pattern_type': 'phase_return'
                        })
        
        # 3. ÏTãƒ†ãƒ³ã‚·ãƒ§ãƒ³å ´ã®æ§‹é€ çš„æŒ¯å‹•
        rho_peaks, _ = find_peaks(rho_T, prominence=np.std(rho_T)*0.5)
        if len(rho_peaks) > 2:
            # ãƒ”ãƒ¼ã‚¯é–“ã®è¦³æ¸¬ã‚¹ãƒ†ãƒƒãƒ—æ•°
            step_intervals = np.diff(rho_peaks)
            unique_intervals, counts = np.unique(step_intervals, return_counts=True)
            
            for interval_steps, count in zip(unique_intervals, counts):
                if count > 2:  # 3å›ä»¥ä¸Šç¹°ã‚Šè¿”ã™ãƒ‘ã‚¿ãƒ¼ãƒ³
                    recurrence_patterns.append({
                        'observation_interval': float(interval_steps),
                        'repetitions': int(count),
                        'structural_coherence': count / len(rho_peaks),
                        'pattern_type': 'tension_oscillation'
                    })
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã®çµ±åˆ
        consolidated = []
        tolerance = 0.15
        
        for pattern in recurrence_patterns:
            found = False
            for c in consolidated:
                # è¦³æ¸¬ã‚¹ãƒ†ãƒƒãƒ—é–“éš”ãŒè¿‘ã„ã‚‚ã®ã‚’çµ±åˆ
                if abs(pattern['observation_interval'] - c['observation_interval']) / c['observation_interval'] < tolerance:
                    c['structural_coherence'] = max(c['structural_coherence'], pattern['structural_coherence'])
                    c['detection_count'] = c.get('detection_count', 1) + 1
                    found = True
                    break
            
            if not found:
                pattern['detection_count'] = 1
                consolidated.append(pattern)
        
        # æ§‹é€ çš„ç¢ºä¿¡åº¦ã®è¨ˆç®—
        for p in consolidated:
            p['topological_confidence'] = p['structural_coherence'] * np.sqrt(p['detection_count'])
        
        consolidated.sort(key=lambda x: x['topological_confidence'], reverse=True)
        
        if self.verbose:
            print(f"   Found {len(consolidated)} recurrence patterns in observation sequence")
            for i, p in enumerate(consolidated[:5]):
                print(f"   {i+1}. Interval: {p['observation_interval']:.0f} steps, "
                      f"Confidence: {p['topological_confidence']:.3f}, "
                      f"Type: {p['pattern_type']}")
        
        return consolidated


    def decompose_structural_signatures(self, structures: Dict[str, np.ndarray],
                                      recurrence_patterns: List[Dict]) -> Dict[str, Dict]:
        """
        è¦³æ¸¬ã‚¹ãƒ†ãƒƒãƒ—åˆ—ã‹ã‚‰æ§‹é€ çš„ã‚·ã‚°ãƒãƒãƒ£ã‚’åˆ†è§£
        """
        if self.verbose:
            print("\nğŸŒ€ Decomposing structural signatures from observation sequence...")
        
        # è¦³æ¸¬ã‚¹ãƒ†ãƒƒãƒ—é…åˆ—
        observation_steps = np.arange(len(structures['positions']))
        
        structural_signatures = {}
        
        for i, pattern in enumerate(recurrence_patterns[:5]):
            if pattern['topological_confidence'] < 0.1:
                break
            
            step_interval = pattern['observation_interval']
            
            # æ§‹é€ çš„ãƒ¢ãƒ‡ãƒ«ï¼ˆè¦³æ¸¬ã‚¹ãƒ†ãƒƒãƒ—ã®é–¢æ•°ã¨ã—ã¦ï¼‰
            def structural_pattern(steps, amplitude, phase_offset):
                # ã‚¹ãƒ†ãƒƒãƒ—é€²è¡Œã«å¯¾ã™ã‚‹æ§‹é€ çš„å¿œç­”
                phase = 2 * np.pi * steps / step_interval + phase_offset
                return amplitude * (np.sin(phase) + 0.3 * np.sin(2*phase))
            
            # æ§‹é€ åï¼ˆè¦³æ¸¬é †åºãƒ™ãƒ¼ã‚¹ï¼‰
            structure_names = ['Primary_Structure', 'Secondary_Structure', 
                            'Tertiary_Structure', 'Quaternary_Structure', 
                            'Quinary_Structure']
            
            structural_signatures[structure_names[i]] = {
                'observation_interval': step_interval,  # è¦³æ¸¬ã‚¹ãƒ†ãƒƒãƒ—é–“éš”
                'topological_amplitude': pattern.get('structural_coherence', 1.0),
                'pattern_type': pattern['pattern_type'],
                'topological_confidence': pattern['topological_confidence'],
                'detection_count': pattern['detection_count']
            }
            
            if self.verbose:
                print(f"   {structure_names[i]}: Every {step_interval:.0f} observation steps")
        
        return structural_signatures


    def analyze(self, data: pd.DataFrame, positions: np.ndarray) -> Dict:
        """
        Complete Pure LambdaÂ³ analysis pipeline.
        è¦³æ¸¬ã‚¹ãƒ†ãƒƒãƒ—åˆ—ã‹ã‚‰ç´”ç²‹ã«ãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«ãªæ§‹é€ ã‚’æŠ½å‡ºï¼
        """
        # 1. LambdaÂ³æ§‹é€ ãƒ†ãƒ³ã‚½ãƒ«ã‚’è¨ˆç®—
        structures = self.compute_lambda_structures(positions)
        
        # 2. æ§‹é€ çš„å¢ƒç•Œã‚’æ¤œå‡ºï¼ˆç‰©ç†å®šæ•°ãªã—ï¼ï¼‰
        boundaries = self.detect_structural_boundaries(structures)
        
        # 3. ãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«ãªç ´ã‚Œã¨ç•°å¸¸ã‚’æ¤œå‡º
        breaks = self.detect_topological_breaks(structures)
        
        # 4. æ§‹é€ çš„å¢ƒç•Œã‚’ä½¿ã£ã¦æ„Ÿåº¦ã‚’å‹•çš„èª¿æ•´
        if boundaries['boundary_locations'].size > 0:
            if self.verbose:
                print("\nğŸ¯ Using structural boundaries to guide detection...")
            
            original_anomaly = breaks['combined_anomaly'].copy()
            boundary_score = boundaries['boundary_score']
            
            if len(boundary_score) < len(original_anomaly):
                padding = len(original_anomaly) - len(boundary_score)
                boundary_score = np.pad(boundary_score, (0, padding), mode='edge')
            
            # å¢ƒç•Œã§ã®æ„Ÿåº¦ã‚’å¢—å¹…
            for i in range(len(original_anomaly)):
                local_boundary = boundary_score[i] if i < len(boundary_score) else 0
                sensitivity = 1.0 + 3.0 * local_boundary
                breaks['combined_anomaly'][i] *= sensitivity
            
            if self.verbose:
                amplification = np.mean(breaks['combined_anomaly']) / np.mean(original_anomaly)
                print(f"   Average sensitivity amplification: {amplification:.2f}x")
        
        # 5. ãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«ãªå†å¸°ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡ºï¼ˆå‘¨æœŸã˜ã‚ƒãªã„ï¼ï¼‰
        recurrence_patterns = self.extract_topological_recurrence(structures)

        # 5.5. é«˜èª¿æ³¢ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼
        recurrence_patterns = self.filter_harmonics_in_recurrence(recurrence_patterns)
        
        # 6. é«˜èª¿æ³¢çš„ãªé–¢ä¿‚ã‚’æŒã¤æ§‹é€ ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        structural_families = self.identify_structural_families(recurrence_patterns)
        
        # å„ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã‹ã‚‰ä»£è¡¨ã‚’é¸æŠ
        representative_patterns = []
        for family_name, patterns in structural_families.items():
            if patterns:
                representative = max(patterns, key=lambda x: x['topological_confidence'])
                representative_patterns.append(representative)
        
        # 7. æ§‹é€ çš„ã‚·ã‚°ãƒãƒãƒ£ã«åˆ†è§£
        structural_signatures = self.decompose_structural_signatures(
            structures, representative_patterns
        )
        
        # 8. ãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¨å®šï¼ˆç‰©ç†ãªã—ï¼ï¼‰
        detected_structures = self.estimate_topological_parameters(
            structural_signatures, structures
        )
        
        # 9. æ˜ã‚‰ã‹ã«åŒã˜æ§‹é€ ã®é«˜èª¿æ³¢ã‚’ãƒãƒ¼ã‚¸
        detected_structures = self.merge_related_structures(detected_structures)
        
        # çµæœã‚’ä¿å­˜
        self.data = data
        self.positions = positions
        self.structures = structures
        self.boundaries = boundaries
        self.breaks = breaks
        self.recurrence_patterns = recurrence_patterns
        self.structural_signatures = structural_signatures
        self.detected_structures = detected_structures
        
        return {
            'n_structures_detected': len(detected_structures),
            'hidden_structures': detected_structures,
            'topological_patterns': structures,
            'topological_breaks': breaks,
            'structural_boundaries': boundaries,
            'observation_steps': len(positions)
        }

    def identify_structural_families(self, patterns: List[Dict]) -> Dict[str, List[Dict]]:
        """
        æ§‹é€ çš„ã«é–¢é€£ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        ï¼ˆè¦³æ¸¬ã‚¹ãƒ†ãƒƒãƒ—é–“éš”ã®æ•´æ•°æ¯”ã§åˆ¤å®šï¼‰
        """
        if not patterns:
            return {}
        
        families = {}
        used = set()
        
        sorted_patterns = sorted(patterns, 
                              key=lambda x: x['topological_confidence'], 
                              reverse=True)
        
        for i, pattern in enumerate(sorted_patterns):
            if i in used:
                continue
                
            family_key = f'structural_family_{i}'
            families[family_key] = [pattern]
            
            # ä»–ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ã®æ•´æ•°æ¯”ã‚’ãƒã‚§ãƒƒã‚¯
            for j, other in enumerate(sorted_patterns[i+1:], i+1):
                if j in used:
                    continue
                
                ratio = pattern['observation_interval'] / other['observation_interval']
                
                # æ•´æ•°æ¯”ãƒã‚§ãƒƒã‚¯ï¼ˆæ§‹é€ çš„é«˜èª¿æ³¢ï¼‰
                for n in [2, 3, 4, 5]:
                    if abs(ratio - n) < 0.05 or abs(ratio - 1/n) < 0.05:
                        families[family_key].append(other)
                        used.add(j)
                        break
        
        return families

    def merge_related_structures(self, structures_list: List[Dict]) -> List[Dict]:
        """
        é–¢é€£ã™ã‚‹æ§‹é€ ã‚’ãƒãƒ¼ã‚¸ï¼ˆé«˜èª¿æ³¢ORè¿‘æ¥å€¤ï¼‰
        """
        if len(structures_list) <= 3:
            return structures_list
        
        merged = []
        used = set()
        
        for i, s1 in enumerate(structures_list):
            if i in used:
                continue
            
            group = [s1]
            
            for j, s2 in enumerate(structures_list[i+1:], i+1):
                if j in used:
                    continue
                
                interval_ratio = s2['observation_interval'] / s1['observation_interval']
                
                # æ¡ä»¶1: é«˜èª¿æ³¢é–¢ä¿‚ï¼ˆæ•´æ•°æ¯”ï¼‰
                is_harmonic = any(
                    abs(interval_ratio - n) < 0.1 or abs(interval_ratio - 1/n) < 0.1
                    for n in [2, 3, 4, 5]
                )
                
                # æ¡ä»¶2: å˜ç´”ã«è¿‘ã„ï¼ˆ20%ä»¥å†…ï¼‰
                is_close = abs(interval_ratio - 1.0) < 0.2
                
                if is_harmonic or is_close:
                    group.append(s2)
                    used.add(j)
                    
                    if self.verbose:
                        if is_harmonic:
                            print(f"   Harmonic relation: {s1['observation_interval']:.0f} & {s2['observation_interval']:.0f}")
                        else:
                            print(f"   Close values: {s1['observation_interval']:.0f} & {s2['observation_interval']:.0f}")
            
            # é«˜èª¿æ³¢ã®å ´åˆã¯åŸºæœ¬æ³¢ã‚’é¸ã¶ã€è¿‘æ¥å€¤ã®å ´åˆã¯åŠ é‡å¹³å‡
            if any(abs(s['observation_interval'] / group[0]['observation_interval'] - n) < 0.1 
                  for s in group[1:] for n in [2, 3, 4, 5]):
                # é«˜èª¿æ³¢ã‚°ãƒ«ãƒ¼ãƒ—ï¼šæœ€ã‚‚é•·ã„é–“éš”ã‚’åŸºæœ¬æ³¢ã¨ã™ã‚‹
                representative = max(group, key=lambda x: x['observation_interval'])
            else:
                # è¿‘æ¥å€¤ã‚°ãƒ«ãƒ¼ãƒ—ï¼šæœ€ã‚‚ç¢ºä¿¡åº¦ã®é«˜ã„ã‚‚ã®ã‚’ä»£è¡¨ã«
                representative = max(group, key=lambda x: x['topological_confidence'])
                
                # åŠ é‡å¹³å‡ã§é–“éš”ã‚’æ›´æ–°
                if len(group) > 1:
                    total_conf = sum(s['topological_confidence'] for s in group)
                    representative['observation_interval'] = sum(
                        s['observation_interval'] * s['topological_confidence'] / total_conf 
                        for s in group
                    )
            
            merged.append(representative)
        
        return merged

    def estimate_topological_parameters(self, 
                                      structural_signatures: Dict[str, Dict],
                                      structures: Dict[str, np.ndarray]) -> List[Dict]:
        """
        ç´”ç²‹ã«ãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¨å®š
        è¦³æ¸¬ã‚¹ãƒ†ãƒƒãƒ—åˆ—ã‹ã‚‰æ§‹é€ çš„ç‰¹æ€§ã‚’æŠ½å‡º
        """
        if self.verbose:
            print("\nğŸŒŒ Estimating topological parameters from observation sequence...")
        
        # ä¸»æ§‹é€ ã®ç‰¹æ€§ï¼ˆè¦³æ¸¬ã‚¹ãƒ†ãƒƒãƒ—ãƒ™ãƒ¼ã‚¹ï¼‰
        positions = structures['positions']
        n_observations = len(positions)
        structural_scale = np.mean(np.linalg.norm(positions, axis=1))
        
        # ä¸»æ§‹é€ ã®å†å¸°é–“éš”ã‚’æ¤œå‡º
        primary_interval = self.detect_primary_recurrence(structures)
        
        if self.verbose:
            print(f"   Observation steps: {n_observations}")
            print(f"   Primary structure: scale={structural_scale:.2f}, recurrence={primary_interval:.0f} steps")
        
        structures_list = []
        
        for name, signature in structural_signatures.items():
            # è¦³æ¸¬ã‚¹ãƒ†ãƒƒãƒ—é–“éš”
            observation_interval = signature['observation_interval']
            
            # æ§‹é€ çš„éšå±¤ï¼ˆä¸»æ§‹é€ ã¨ã®é–¢ä¿‚ï¼‰
            if observation_interval > primary_interval:
                hierarchy_factor = observation_interval / primary_interval
            else:
                hierarchy_factor = primary_interval / observation_interval
            
            # ãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«åŠå¾„ï¼ˆæ§‹é€ çš„ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
            relative_scale = (observation_interval / primary_interval) ** (2/3)
            topological_radius = structural_scale * relative_scale
            
            # æ§‹é€ çš„å½±éŸ¿åŠ›
            structural_influence = signature.get('topological_amplitude', 1.0) * structural_scale**2
            
            structure_params = {
                'name': name,
                'observation_interval': observation_interval,
                'hierarchy_factor': hierarchy_factor,
                'topological_radius': topological_radius,
                'structural_influence': structural_influence,
                'topological_impact': signature.get('topological_amplitude', 1.0),
                'topological_confidence': signature['topological_confidence'],
                'pattern_type': signature['pattern_type'],
                'detection_count': signature.get('detection_count', 1)
            }
            
            structures_list.append(structure_params)
        
        # ãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«åŠå¾„ã§ã‚½ãƒ¼ãƒˆ
        structures_list.sort(key=lambda x: x['topological_radius'])
        
        # ä½ç¢ºä¿¡åº¦ã‚’é™¤å¤–
        structures_list = [
            s for s in structures_list 
            if s['topological_confidence'] > 0.05
        ]
        
        return structures_list

    def detect_primary_recurrence(self, structures: Dict[str, np.ndarray]) -> float:
        """
        ä¸»æ§‹é€ ã®å†å¸°é–“éš”ã‚’æ¤œå‡ºï¼ˆè¦³æ¸¬ã‚¹ãƒ†ãƒƒãƒ—å˜ä½ï¼‰
        """
        positions = structures['positions']
        n_steps = len(positions)
        
        # Method 1: Q_Î›ã®å·»ãæ•°ã‹ã‚‰
        if 'Q_cumulative' in structures:
            Q_final = structures['Q_cumulative'][-1]
            topological_winding = abs(Q_final)
            
            if topological_winding > 0.5:
                recurrence_interval = n_steps / topological_winding
                if n_steps * 0.3 < recurrence_interval < n_steps * 0.7:
                    return recurrence_interval
        
        # Method 2: æ§‹é€ çš„è‡ªå·±ç›¸ä¼¼æ€§
        structural_distances = np.linalg.norm(positions, axis=1)
        pattern = structural_distances - np.mean(structural_distances)
        
        # è‡ªå·±ç›¸é–¢ã§å†å¸°ã‚’æ¤œå‡º
        min_lag = int(n_steps * 0.05)  # 5%ä»¥ä¸Š
        max_lag = int(n_steps * 0.7)   # 70%ä»¥ä¸‹
        
        autocorr = correlate(pattern, pattern, mode='full')
        autocorr = autocorr[len(autocorr)//2:]
        autocorr = autocorr / autocorr[0]
        
        if max_lag > min_lag:
            peaks, _ = find_peaks(autocorr[min_lag:max_lag], height=0.3)
            if len(peaks) > 0:
                return float(peaks[0] + min_lag)
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        return float(n_steps // 3)

    def print_results(self):
        """è¦³æ¸¬ã‚¹ãƒ†ãƒƒãƒ—ãƒ™ãƒ¼ã‚¹ã§çµæœã‚’è¡¨ç¤ºï¼ˆå…ƒã®æƒ‘æ˜Ÿã¨ã®å¯¾å¿œä»˜ãï¼‰"""
        print("\n" + "="*70)
        print("ğŸŒŒ Pure LambdaÂ³ Topological Analysis Results")
        print("="*70)
        print("\nâš¡ NO PHYSICS! Only pure topological structure!")
        print(f"ğŸ“Š Total observation steps: {len(self.positions)}")
        
        # Kurisu's simulation parameters (for reference)
        print("\nğŸ“ Reference: Makise Kurisu's Original Universe")
        print("   'A new universe emerges... with secrets, noise, and missingness!'")
        
        # æœŸå¾…ã•ã‚Œã‚‹å‘¨æœŸï¼ˆã‚±ãƒ—ãƒ©ãƒ¼ã®ç¬¬3æ³•å‰‡ã‹ã‚‰æ¦‚ç®—ï¼‰
        expected_periods = {
            'alpha': {'a': 1.2, 'period': 480, 'mass': 1e-5},
            'X': {'a': 2.0, 'period': 923, 'mass': 2e-5},  
            'Y': {'a': 2.5, 'period': 1435, 'mass': 8e-6},
            'Z': {'a': 3.4, 'period': 2274, 'mass': 6e-6}
        }
        
        print("\n   Expected structures from simulation:")
        for name, params in expected_periods.items():
            print(f"   - Planet {name}: a={params['a']} AU, Tâ‰ˆ{params['period']} days, M={params['mass']}")
        
        if hasattr(self, 'boundaries') and self.boundaries['boundary_locations'].size > 0:
            print(f"\nğŸŒŸ Structural Boundaries: {len(self.boundaries['boundary_locations'])}")
            print("   (Natural topological limits in observation sequence)")
        
        print(f"\nğŸ” Detected {len(self.detected_structures)} hidden structures:")
        print("-"*70)
        
        # ãƒãƒƒãƒãƒ³ã‚°çµæœ
        matched_count = 0
        unmatched_structures = []
        
        for i, structure in enumerate(self.detected_structures):
            print(f"\n{structure['name']}:")
            print(f"  Observation interval: {structure['observation_interval']:.0f} steps")
            print(f"  Hierarchy factor: {structure['hierarchy_factor']:.2f}")
            print(f"  Topological radius: {structure['topological_radius']:.2f}")
            print(f"  Structural influence: {structure['structural_influence']:.0f}")
            print(f"  Detection confidence: {structure['topological_confidence']:.3f}")
            print(f"  Pattern type: {structure['pattern_type']}")
            
            # ã©ã®æƒ‘æ˜Ÿã«ãƒãƒƒãƒã™ã‚‹ã‹åˆ¤å®š
            best_match = None
            best_diff = float('inf')
            
            for planet_name, params in expected_periods.items():
                diff = abs(structure['observation_interval'] - params['period']) / params['period']
                if diff < best_diff and diff < 0.2:  # 20%ä»¥å†…
                    best_diff = diff
                    best_match = planet_name
            
            if best_match:
                matched_count += 1
                print(f"  âœ… MATCHED: Planet {best_match} "
                      f"(expected: {expected_periods[best_match]['period']} days, "
                      f"diff: {best_diff*100:.1f}%)")
                print(f"     Original params: a={expected_periods[best_match]['a']} AU, "
                      f"M={expected_periods[best_match]['mass']}")
            else:
                unmatched_structures.append(structure)
                print(f"  â“ No clear match to simulation planets")
        
        # ã‚µãƒãƒªãƒ¼
        print("\n" + "="*70)
        print("ğŸ“Š MATCHING SUMMARY:")
        print(f"   Matched: {matched_count}/{len(expected_periods)} planets")
        
        # ã©ã®æƒ‘æ˜ŸãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸã‹
        detected_planets = set()
        for structure in self.detected_structures:
            for planet_name, params in expected_periods.items():
                diff = abs(structure['observation_interval'] - params['period']) / params['period']
                if diff < 0.2:
                    detected_planets.add(planet_name)
        
        missing_planets = set(expected_periods.keys()) - detected_planets
        if missing_planets:
            print(f"   Missing: {', '.join(f'Planet {p}' for p in sorted(missing_planets))}")
            for planet in missing_planets:
                print(f"      â†’ Planet {planet} (Tâ‰ˆ{expected_periods[planet]['period']} days) not detected")
                if expected_periods[planet]['period'] > len(self.positions):
                    print(f"         (Period longer than observation sequence!)")
        
        print("\nğŸ¯ LambdaÂ³ SUCCESS: Hidden structures revealed through pure topology!")
        print("   Transaction, not time. Structure, not physics!")
        print("   But we found Kurisu's hidden planets! ğŸŒŸ")
        print("="*70)

    def main():
        """Main execution function - Pure LambdaÂ³ with observation steps only!"""
        parser = argparse.ArgumentParser(
            description='Pure LambdaÂ³ Framework - Topological Structure Detection from Observation Sequence'
        )
        parser.add_argument(
            '--data', 
            type=str, 
            default='challenge_blackhole_alpha_noisy.csv',
            help='Path to CSV file containing observation sequence'
        )
        parser.add_argument(
            '--save-plot',
            type=str,
            default=None,
            help='Path to save analysis plot'
        )
        parser.add_argument(
            '--quiet',
            action='store_true',
            help='Suppress verbose output'
        )
        parser.add_argument(
            '--max-structures',
            type=int,
            default=5,
            help='Maximum number of structures to detect (default: 5)'
        )
        
        args = parser.parse_args()
        
        print("\nâœ¨ Pure LambdaÂ³ Analysis - Transaction-based Reality âœ¨")
        print("=" * 60)
        print("NO TIME. NO PHYSICS. ONLY STRUCTURE.")
        print("=" * 60)
        
        # Initialize analyzer
        analyzer = PureLambda3Analyzer(verbose=not args.quiet)
        
        # Load observation sequence
        data, positions = analyzer.load_and_clean_data(args.data)
        
        # Validate observation sequence
        n_observations = len(positions)
        print(f"\nğŸ“Š Observation sequence loaded:")
        print(f"   Total steps: {n_observations}")
        print(f"   Missing data interpolated: âœ“")
        
        if n_observations < 500:
            print("\nâš ï¸  Warning: Short observation sequence detected!")
            print(f"   Recommended: >500 steps, Got: {n_observations} steps")
            print("   Results may be less reliable.")
        
        # Run pure topological analysis
        print("\nğŸŒŒ Starting LambdaÂ³ analysis...")
        results = analyzer.analyze(data, positions)
        
        # Print results
        analyzer.print_results()
        
        # Additional summary
        print("\nğŸ“ˆ Analysis Summary:")
        print(f"   Observation steps analyzed: {results['observation_steps']}")
        print(f"   Structures detected: {results['n_structures_detected']}")
        print(f"   Structural boundaries found: {len(results['structural_boundaries']['boundary_locations'])}")
        
        # Plot results
        if not args.quiet:
            print("\nğŸ“Š Generating visualization...")
            analyzer.plot_results(save_path=args.save_plot)
        
        # Export results
        export_results(analyzer, args.data)
        
        print("\nâœ¨ LambdaÂ³ analysis complete!")
        print("   The hidden structure has been revealed through pure topology!")
        print("   Remember: Time is an illusion. Only Transaction exists! ğŸŒ€")

    def plot_results(self, save_path: Optional[str] = None):
        """Visualization of Pure LambdaÂ³ analysis - observation step based"""
        import matplotlib.pyplot as plt
        
        fig = plt.figure(figsize=(18, 14))
        
        # 1. è¦³æ¸¬è»Œè·¡
        ax1 = plt.subplot(3, 4, 1)
        ax1.plot(self.positions[:, 0], self.positions[:, 1], 
                'k-', linewidth=0.5, alpha=0.7)
        ax1.scatter(0, 0, color='orange', s=200, marker='*', label='Center')
        ax1.set_xlabel('X [structural units]')
        ax1.set_ylabel('Y [structural units]')
        ax1.set_title('Observation Trajectory')
        ax1.axis('equal')
        ax1.grid(True, alpha=0.3)
        
        # 2. ãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«ãƒãƒ£ãƒ¼ã‚¸ã®ç´¯ç©
        ax2 = plt.subplot(3, 4, 2)
        steps = np.arange(len(self.structures['Q_cumulative']))
        ax2.plot(steps, self.structures['Q_cumulative'], 'b-', linewidth=2)
        ax2.set_xlabel('Observation Steps')
        ax2.set_ylabel('Q_Î› (cumulative)')
        ax2.set_title('Topological Winding')
        ax2.grid(True, alpha=0.3)
        
        # 3. è¤‡åˆç•°å¸¸ã‚¹ã‚³ã‚¢
        ax3 = plt.subplot(3, 4, 3)
        anomaly_steps = np.arange(len(self.breaks['combined_anomaly']))
        ax3.plot(anomaly_steps, self.breaks['combined_anomaly'], 'r-', alpha=0.7)
        ax3.set_xlabel('Observation Steps')
        ax3.set_ylabel('Anomaly Score')
        ax3.set_title('Topological Breaks')
        ax3.grid(True, alpha=0.3)
        
        # 4. æ§‹é€ çš„å¢ƒç•Œ
        ax4 = plt.subplot(3, 4, 4)
        boundary_steps = np.arange(len(self.boundaries['boundary_score']))
        ax4.plot(boundary_steps, self.boundaries['boundary_score'], 'purple', alpha=0.7)
        for boundary in self.boundaries['boundary_locations']:
            ax4.axvline(boundary, color='red', linestyle='--', alpha=0.5)
        ax4.set_xlabel('Observation Steps')
        ax4.set_ylabel('Boundary Score')
        ax4.set_title('Structural Boundaries')
        ax4.grid(True, alpha=0.3)
        
        # 5. Î›F magnitude
        ax5 = plt.subplot(3, 4, 5)
        lf_steps = np.arange(len(self.structures['lambda_F_mag']))
        ax5.plot(lf_steps, self.structures['lambda_F_mag'], 'g-', alpha=0.7)
        ax5.set_xlabel('Observation Steps')
        ax5.set_ylabel('|Î›F|')
        ax5.set_title('Structural Flow Magnitude')
        ax5.grid(True, alpha=0.3)
        
        # 6. ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å ´
        ax6 = plt.subplot(3, 4, 6)
        rho_steps = np.arange(len(self.structures['rho_T']))
        ax6.plot(rho_steps, self.structures['rho_T'], 'm-', alpha=0.7)
        ax6.set_xlabel('Observation Steps')
        ax6.set_ylabel('ÏT')
        ax6.set_title('Structural Tension Field')
        ax6.grid(True, alpha=0.3)
        
        # 7. æ¤œå‡ºã•ã‚ŒãŸå†å¸°ãƒ‘ã‚¿ãƒ¼ãƒ³
        ax7 = plt.subplot(3, 4, 7)
        if hasattr(self, 'recurrence_patterns') and self.recurrence_patterns:
            intervals = [p['observation_interval'] for p in self.recurrence_patterns[:10]]
            confidences = [p['topological_confidence'] for p in self.recurrence_patterns[:10]]
            y_pos = np.arange(len(intervals))
            
            ax7.barh(y_pos, intervals, color='cyan', alpha=0.7)
            ax7.set_yticks(y_pos)
            ax7.set_yticklabels([f"Pattern {i+1}" for i in range(len(intervals))])
            ax7.set_xlabel('Recurrence Interval [steps]')
            ax7.set_title('Detected Recurrence Patterns')
            
            # Confidence as text
            for i, (interval, conf) in enumerate(zip(intervals, confidences)):
                ax7.text(interval + 20, i, f'{conf:.1f}', va='center')
        
        # 8. Q_residual
        ax8 = plt.subplot(3, 4, 8)
        q_steps = np.arange(len(self.breaks['Q_residual']))
        ax8.plot(q_steps, self.breaks['Q_residual'], 'c-', alpha=0.7)
        ax8.set_xlabel('Observation Steps')
        ax8.set_ylabel('Q_Î› Residual')
        ax8.set_title('Topological Charge Anomaly')
        ax8.grid(True, alpha=0.3)
        
        # 9. æ¤œå‡ºçµæœã‚µãƒãƒªãƒ¼
        ax9 = plt.subplot(3, 4, 9)
        ax9.axis('off')
        
        summary = "ğŸŒŸ Pure LambdaÂ³ Detection Results\n" + "="*40 + "\n\n"
        summary += "NO TIME. NO PHYSICS. ONLY STRUCTURE.\n\n"
        summary += f"Total observation steps: {len(self.positions)}\n"
        summary += f"Structural boundaries: {len(self.boundaries['boundary_locations'])}\n"
        summary += f"Detected structures: {len(self.detected_structures)}\n\n"
        
        for structure in self.detected_structures[:3]:
            summary += f"{structure['name']}:\n"
            summary += f"  Interval: {structure['observation_interval']:.0f} steps\n"
            summary += f"  Confidence: {structure['topological_confidence']:.1f}\n\n"
        
        ax9.text(0.1, 0.9, summary, transform=ax9.transAxes,
                fontsize=10, verticalalignment='top', fontfamily='monospace')
        
        # 10. ãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«æ§‹é€ å›³
        ax10 = plt.subplot(3, 4, 10)
        theta = np.linspace(0, 2*np.pi, 100)
        
        # ä¸»æ§‹é€ 
        r_primary = np.mean(np.linalg.norm(self.positions, axis=1))
        ax10.plot(r_primary * np.cos(theta), r_primary * np.sin(theta),
                'k--', alpha=0.5, label='Primary')
        
        # æ¤œå‡ºã•ã‚ŒãŸæ§‹é€ 
        colors = ['r', 'g', 'b', 'c', 'm', 'y']
        for i, structure in enumerate(self.detected_structures[:5]):
            r = structure['topological_radius']
            ax10.plot(r * np.cos(theta), r * np.sin(theta),
                    color=colors[i % len(colors)], linestyle='--', alpha=0.5,
                    label=f"{structure['name']} ({structure['observation_interval']:.0f})")
        
        ax10.scatter(0, 0, color='orange', s=200, marker='*')
        ax10.set_xlabel('X [structural units]')
        ax10.set_ylabel('Y [structural units]')
        ax10.set_title('Topological Architecture')
        ax10.legend(fontsize=8)
        ax10.axis('equal')
        ax10.grid(True, alpha=0.3)
        
        # 11. ãƒ˜ãƒªã‚·ãƒ†ã‚£
        ax11 = plt.subplot(3, 4, 11)
        hel_steps = np.arange(len(self.structures['helicity']))
        ax11.plot(hel_steps, self.structures['helicity'], 'y-', alpha=0.7)
        ax11.set_xlabel('Observation Steps')
        ax11.set_ylabel('Helicity')
        ax11.set_title('Structural Helicity')
        ax11.grid(True, alpha=0.3)
        
        # 12. ä½ç›¸ç©ºé–“ï¼ˆç•°å¸¸åº¦ã§è‰²ä»˜ã‘ï¼‰
        ax12 = plt.subplot(3, 4, 12)
        n_points = min(len(self.positions)-1, 
                      len(self.structures['lambda_F']), 
                      len(self.breaks['combined_anomaly']))
        
        scatter = ax12.scatter(self.positions[:n_points, 0], 
                            self.structures['lambda_F'][:n_points, 0],
                            c=self.breaks['combined_anomaly'][:n_points],
                            cmap='plasma', s=1, alpha=0.7)
        plt.colorbar(scatter, ax=ax12, label='Anomaly')
        ax12.set_xlabel('X [structural units]')
        ax12.set_ylabel('Î›F_x [Î”structure/step]')
        ax12.set_title('Phase Space (colored by anomaly)')
        ax12.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            if self.verbose:
                print(f"\nğŸ“Š Figure saved to {save_path}")
        
        plt.show()    

    def export_results(analyzer, input_filename):
        """Export analysis results to files"""
        import json
        from datetime import datetime
        
        # Create output filename
        base_name = input_filename.replace('.csv', '')
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # Export detected structures
        structures_data = []
        for structure in analyzer.detected_structures:
            structures_data.append({
                'name': structure['name'],
                'observation_interval_steps': float(structure['observation_interval']),
                'hierarchy_factor': float(structure['hierarchy_factor']),
                'topological_radius': float(structure['topological_radius']),
                'structural_influence': float(structure['structural_influence']),
                'confidence': float(structure['topological_confidence']),
                'pattern_type': structure['pattern_type']
            })
        
        output_data = {
            'analysis_type': 'Pure LambdaÂ³ Topological Analysis',
            'observation_steps': len(analyzer.positions),
            'structures_detected': len(structures_data),
            'structural_boundaries': len(analyzer.boundaries['boundary_locations']),
            'primary_recurrence_interval': float(analyzer.detect_primary_recurrence(analyzer.structures)),
            'detected_structures': structures_data,
            'metadata': {
                'input_file': input_filename,
                'analysis_timestamp': timestamp,
                'lambda3_version': '1.0.0-pure'
            }
        }
        
        # Save JSON
        output_file = f"{base_name}_lambda3_results_{timestamp}.json"
        with open(output_file, 'w') as f:
            json.dump(output_data, f, indent=2)
        
        print(f"\nğŸ’¾ Results exported to: {output_file}")

# For Jupyter/Colab compatibility
if __name__ == "__main__":
    import sys
    
    # Check if running in notebook
    try:
        get_ipython()
        in_notebook = True
    except NameError:
        in_notebook = False
    
    if in_notebook:
        print("ğŸŒŸ Running in notebook mode...")
        print("   Creating analyzer with default parameters")
        
        # Notebook-friendly initialization
        analyzer = PureLambda3Analyzer(verbose=True)
        
        # Check for data file
        import os
        default_file = 'challenge_blackhole_alpha_noisy.csv'
        
        if os.path.exists(default_file):
            print(f"\nğŸ“Š Loading {default_file}...")
            data, positions = analyzer.load_and_clean_data(default_file)
            
            print("\nğŸŒŒ Running analysis...")
            results = analyzer.analyze(data, positions)
            
            analyzer.print_results()
            analyzer.plot_results()
        else:
            print("\nâš ï¸ Data file not found!")
            print(f"   Please upload: {default_file}")
            print("   Or generate test data first.")
            
            # Optionally generate test data
            print("\nğŸ’¡ To generate test data, run:")
            print("   analyzer.generate_test_data('test_data.csv', n_steps=1500)")
    else:
        # Standard command-line execution
        try:
            main()
        except KeyboardInterrupt:
            print("\n\nâš¡ Analysis interrupted by user")
            sys.exit(0)
        except Exception as e:
            print(f"\nâŒ Error occurred: {str(e)}")
            import traceback
            traceback.print_exc()
            sys.exit(1)
